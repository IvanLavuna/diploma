{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:20:39.145523949Z",
     "start_time": "2024-01-20T08:20:38.035624167Z"
    }
   },
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from numpy import sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, max_error, median_absolute_error, mean_absolute_error\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(f'Mean Squared Error (MSE):              {mse:.10f}')\n",
    "    \n",
    "    rmse = sqrt(mse)\n",
    "    print(f'Root Mean Squared Error (RMSE):        {rmse:.10f}')\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f'Mean Absolute Error (MAE):             {mae:.10f}')\n",
    "    \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f'R-squared (R²):                        {r2:.10f}')\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    print(f'Mean Absolute Percentage Error (MAPE): {mape:.10f}')    \n",
    "    \n",
    "    me = max_error(y_true, y_pred)\n",
    "    print(f'Max Error (ME):                        {me:.10f}')    \n",
    "    \n",
    "    med_ae = median_absolute_error(y_true, y_pred)\n",
    "    print(f'Median Absolute Error (MedAE):         {med_ae:.10f}') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:31:25.100670739Z",
     "start_time": "2024-01-20T08:31:25.059841362Z"
    }
   },
   "id": "2cafda65f0655a23",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 32/42 [00:05<00:03,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileRegressor model failed to execute\n",
      "Solver interior-point is not anymore available in SciPy >= 1.11.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:08<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2813\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 36837674.237624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               Adjusted R-Squared  R-Squared        RMSE  \\\nModel                                                                      \nExtraTreesRegressor                          0.87       0.88  5461130.73   \nGradientBoostingRegressor                    0.87       0.87  5540243.04   \nRandomForestRegressor                        0.87       0.87  5622548.77   \nXGBRegressor                                 0.86       0.86  5767130.50   \nLGBMRegressor                                0.86       0.86  5818060.02   \nLassoCV                                      0.85       0.86  5870287.05   \nRidgeCV                                      0.85       0.86  5872683.54   \nRidge                                        0.85       0.86  5872684.06   \nLassoLarsCV                                  0.85       0.86  5873636.96   \nLassoLars                                    0.85       0.86  5874620.88   \nLasso                                        0.85       0.86  5874621.68   \nLassoLarsIC                                  0.85       0.86  5874622.10   \nLinearRegression                             0.85       0.86  5874622.25   \nTransformedTargetRegressor                   0.85       0.86  5874622.25   \nBaggingRegressor                             0.85       0.86  5897418.47   \nSGDRegressor                                 0.85       0.86  5913318.15   \nOrthogonalMatchingPursuitCV                  0.85       0.86  5929242.48   \nLarsCV                                       0.85       0.85  5968656.61   \nHistGradientBoostingRegressor                0.85       0.85  5987785.51   \nOrthogonalMatchingPursuit                    0.84       0.85  6099081.51   \nElasticNet                                   0.83       0.83  6398123.22   \nHuberRegressor                               0.82       0.83  6467916.14   \nRANSACRegressor                              0.82       0.82  6609021.71   \nKNeighborsRegressor                          0.80       0.80  6962130.61   \nTweedieRegressor                             0.79       0.79  7103535.54   \nExtraTreeRegressor                           0.78       0.78  7264047.08   \nDecisionTreeRegressor                        0.77       0.77  7458474.42   \nAdaBoostRegressor                            0.76       0.77  7560453.15   \nPoissonRegressor                             0.73       0.74  7944306.90   \nGammaRegressor                               0.68       0.69  8666284.28   \nElasticNetCV                                -0.04      -0.01 15667809.95   \nBayesianRidge                               -0.04      -0.01 15670627.81   \nDummyRegressor                              -0.04      -0.01 15670627.95   \nNuSVR                                       -0.10      -0.07 16153260.48   \nSVR                                         -0.14      -0.10 16402565.40   \nLars                                        -0.85      -0.79 20926671.75   \nGaussianProcessRegressor                    -0.85      -0.79 20934021.71   \nKernelRidge                                 -5.01      -4.83 37748946.43   \nPassiveAggressiveRegressor                  -5.62      -5.42 39596454.27   \nMLPRegressor                                -6.12      -5.91 41084692.75   \nLinearSVR                                   -6.12      -5.91 41084726.91   \n\n                               Time Taken  \nModel                                      \nExtraTreesRegressor                  0.45  \nGradientBoostingRegressor            0.54  \nRandomForestRegressor                1.15  \nXGBRegressor                         0.21  \nLGBMRegressor                        0.08  \nLassoCV                              0.20  \nRidgeCV                              0.03  \nRidge                                0.01  \nLassoLarsCV                          0.08  \nLassoLars                            0.04  \nLasso                                0.06  \nLassoLarsIC                          0.03  \nLinearRegression                     0.03  \nTransformedTargetRegressor           0.01  \nBaggingRegressor                     0.13  \nSGDRegressor                         0.02  \nOrthogonalMatchingPursuitCV          0.02  \nLarsCV                               0.07  \nHistGradientBoostingRegressor        0.22  \nOrthogonalMatchingPursuit            0.01  \nElasticNet                           0.02  \nHuberRegressor                       0.02  \nRANSACRegressor                      0.06  \nKNeighborsRegressor                  0.02  \nTweedieRegressor                     0.66  \nExtraTreeRegressor                   0.02  \nDecisionTreeRegressor                0.03  \nAdaBoostRegressor                    0.14  \nPoissonRegressor                     1.44  \nGammaRegressor                       0.26  \nElasticNetCV                         0.12  \nBayesianRidge                        0.01  \nDummyRegressor                       0.01  \nNuSVR                                0.14  \nSVR                                  0.18  \nLars                                 0.03  \nGaussianProcessRegressor             0.26  \nKernelRidge                          0.12  \nPassiveAggressiveRegressor           0.16  \nMLPRegressor                         1.26  \nLinearSVR                            0.02  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adjusted R-Squared</th>\n      <th>R-Squared</th>\n      <th>RMSE</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.87</td>\n      <td>0.88</td>\n      <td>5461130.73</td>\n      <td>0.45</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.87</td>\n      <td>0.87</td>\n      <td>5540243.04</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.87</td>\n      <td>0.87</td>\n      <td>5622548.77</td>\n      <td>1.15</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.86</td>\n      <td>0.86</td>\n      <td>5767130.50</td>\n      <td>0.21</td>\n    </tr>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.86</td>\n      <td>0.86</td>\n      <td>5818060.02</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>LassoCV</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5870287.05</td>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>RidgeCV</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5872683.54</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5872684.06</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LassoLarsCV</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5873636.96</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>LassoLars</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5874620.88</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5874621.68</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>LassoLarsIC</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5874622.10</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5874622.25</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>TransformedTargetRegressor</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5874622.25</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>BaggingRegressor</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5897418.47</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>SGDRegressor</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5913318.15</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuitCV</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5929242.48</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LarsCV</th>\n      <td>0.85</td>\n      <td>0.85</td>\n      <td>5968656.61</td>\n      <td>0.07</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingRegressor</th>\n      <td>0.85</td>\n      <td>0.85</td>\n      <td>5987785.51</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuit</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>6099081.51</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>ElasticNet</th>\n      <td>0.83</td>\n      <td>0.83</td>\n      <td>6398123.22</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>HuberRegressor</th>\n      <td>0.82</td>\n      <td>0.83</td>\n      <td>6467916.14</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>RANSACRegressor</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6609021.71</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.80</td>\n      <td>0.80</td>\n      <td>6962130.61</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>TweedieRegressor</th>\n      <td>0.79</td>\n      <td>0.79</td>\n      <td>7103535.54</td>\n      <td>0.66</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeRegressor</th>\n      <td>0.78</td>\n      <td>0.78</td>\n      <td>7264047.08</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7458474.42</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>7560453.15</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>PoissonRegressor</th>\n      <td>0.73</td>\n      <td>0.74</td>\n      <td>7944306.90</td>\n      <td>1.44</td>\n    </tr>\n    <tr>\n      <th>GammaRegressor</th>\n      <td>0.68</td>\n      <td>0.69</td>\n      <td>8666284.28</td>\n      <td>0.26</td>\n    </tr>\n    <tr>\n      <th>ElasticNetCV</th>\n      <td>-0.04</td>\n      <td>-0.01</td>\n      <td>15667809.95</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>BayesianRidge</th>\n      <td>-0.04</td>\n      <td>-0.01</td>\n      <td>15670627.81</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DummyRegressor</th>\n      <td>-0.04</td>\n      <td>-0.01</td>\n      <td>15670627.95</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NuSVR</th>\n      <td>-0.10</td>\n      <td>-0.07</td>\n      <td>16153260.48</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>-0.14</td>\n      <td>-0.10</td>\n      <td>16402565.40</td>\n      <td>0.18</td>\n    </tr>\n    <tr>\n      <th>Lars</th>\n      <td>-0.85</td>\n      <td>-0.79</td>\n      <td>20926671.75</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>GaussianProcessRegressor</th>\n      <td>-0.85</td>\n      <td>-0.79</td>\n      <td>20934021.71</td>\n      <td>0.26</td>\n    </tr>\n    <tr>\n      <th>KernelRidge</th>\n      <td>-5.01</td>\n      <td>-4.83</td>\n      <td>37748946.43</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveRegressor</th>\n      <td>-5.62</td>\n      <td>-5.42</td>\n      <td>39596454.27</td>\n      <td>0.16</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>-6.12</td>\n      <td>-5.91</td>\n      <td>41084692.75</td>\n      <td>1.26</td>\n    </tr>\n    <tr>\n      <th>LinearSVR</th>\n      <td>-6.12</td>\n      <td>-5.91</td>\n      <td>41084726.91</td>\n      <td>0.02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_name, y2_name, y3_name = \"dir_costs\",\"traffic_costs_s_r\",\"lost_trips_costs_s_r\" \n",
    "\n",
    "df = pd.read_csv('data/dataset.csv')\n",
    "\n",
    "X = df.drop(columns=[y1_name, y2_name, y3_name], axis=1).values\n",
    "y = df[y1_name].values\n",
    "\n",
    "X, y = shuffle(X, y, random_state=13)\n",
    "\n",
    "# Convert X to float32\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "offset = int(X.shape[0] * 0.8)\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "\n",
    "# LazyRegressor part\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:20:47.591621860Z",
     "start_time": "2024-01-20T08:20:39.149420493Z"
    }
   },
   "id": "651cadc1eaadb2ee",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2813\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 2414681.912529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               Adjusted R-Squared  R-Squared        RMSE  \\\nModel                                                                      \nGradientBoostingRegressor                    0.88       0.89  5004093.60   \nExtraTreesRegressor                          0.88       0.88  5146156.09   \nRandomForestRegressor                        0.87       0.88  5153260.34   \nBaggingRegressor                             0.87       0.87  5269350.87   \nHistGradientBoostingRegressor                0.87       0.87  5279396.63   \nLGBMRegressor                                0.86       0.87  5391437.07   \nXGBRegressor                                 0.85       0.86  5573352.38   \nLars                                         0.82       0.83  6169248.03   \nLarsCV                                       0.82       0.82  6191434.93   \nTransformedTargetRegressor                   0.82       0.82  6205560.71   \nLinearRegression                             0.82       0.82  6205560.71   \nLassoLars                                    0.82       0.82  6205561.64   \nLasso                                        0.82       0.82  6205562.63   \nRidge                                        0.82       0.82  6212954.45   \nLassoLarsCV                                  0.82       0.82  6224992.54   \nLassoCV                                      0.82       0.82  6245111.33   \nLassoLarsIC                                  0.82       0.82  6245114.60   \nRidgeCV                                      0.82       0.82  6252845.39   \nSGDRegressor                                 0.81       0.82  6266456.70   \nOrthogonalMatchingPursuit                    0.81       0.82  6339506.90   \nOrthogonalMatchingPursuitCV                  0.81       0.82  6339506.90   \nAdaBoostRegressor                            0.80       0.81  6438796.80   \nRANSACRegressor                              0.79       0.80  6647072.24   \nKernelRidge                                  0.78       0.79  6770052.74   \nExtraTreeRegressor                           0.75       0.75  7344041.61   \nHuberRegressor                               0.72       0.73  7749711.13   \nElasticNet                                   0.71       0.71  7901383.93   \nDecisionTreeRegressor                        0.69       0.70  8073960.79   \nKNeighborsRegressor                          0.66       0.67  8441281.89   \nTweedieRegressor                             0.59       0.60  9361034.29   \nGaussianProcessRegressor                     0.54       0.55  9923535.54   \nPassiveAggressiveRegressor                   0.08       0.11 13971470.22   \nElasticNetCV                                -0.03      -0.00 14797869.00   \nBayesianRidge                               -0.03      -0.00 14798754.25   \nDummyRegressor                              -0.03      -0.00 14798754.26   \nNuSVR                                       -0.04      -0.01 14840111.61   \nSVR                                         -0.04      -0.01 14857857.90   \nLinearSVR                                   -0.07      -0.04 15079955.18   \nMLPRegressor                                -0.07      -0.04 15080527.14   \n\n                               Time Taken  \nModel                                      \nGradientBoostingRegressor            0.58  \nExtraTreesRegressor                  0.57  \nRandomForestRegressor                1.10  \nBaggingRegressor                     0.12  \nHistGradientBoostingRegressor        0.22  \nLGBMRegressor                        0.06  \nXGBRegressor                         0.22  \nLars                                 0.02  \nLarsCV                               0.07  \nTransformedTargetRegressor           0.01  \nLinearRegression                     0.02  \nLassoLars                            0.02  \nLasso                                0.04  \nRidge                                0.01  \nLassoLarsCV                          0.06  \nLassoCV                              0.15  \nLassoLarsIC                          0.02  \nRidgeCV                              0.04  \nSGDRegressor                         0.03  \nOrthogonalMatchingPursuit            0.01  \nOrthogonalMatchingPursuitCV          0.01  \nAdaBoostRegressor                    0.11  \nRANSACRegressor                      0.06  \nKernelRidge                          0.08  \nExtraTreeRegressor                   0.02  \nHuberRegressor                       0.01  \nElasticNet                           0.04  \nDecisionTreeRegressor                0.03  \nKNeighborsRegressor                  0.02  \nTweedieRegressor                     0.60  \nGaussianProcessRegressor             0.20  \nPassiveAggressiveRegressor           0.17  \nElasticNetCV                         0.13  \nBayesianRidge                        0.01  \nDummyRegressor                       0.01  \nNuSVR                                0.14  \nSVR                                  0.18  \nLinearSVR                            0.02  \nMLPRegressor                         1.10  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adjusted R-Squared</th>\n      <th>R-Squared</th>\n      <th>RMSE</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.88</td>\n      <td>0.89</td>\n      <td>5004093.60</td>\n      <td>0.58</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.88</td>\n      <td>0.88</td>\n      <td>5146156.09</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.87</td>\n      <td>0.88</td>\n      <td>5153260.34</td>\n      <td>1.10</td>\n    </tr>\n    <tr>\n      <th>BaggingRegressor</th>\n      <td>0.87</td>\n      <td>0.87</td>\n      <td>5269350.87</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingRegressor</th>\n      <td>0.87</td>\n      <td>0.87</td>\n      <td>5279396.63</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.86</td>\n      <td>0.87</td>\n      <td>5391437.07</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5573352.38</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>Lars</th>\n      <td>0.82</td>\n      <td>0.83</td>\n      <td>6169248.03</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LarsCV</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6191434.93</td>\n      <td>0.07</td>\n    </tr>\n    <tr>\n      <th>TransformedTargetRegressor</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6205560.71</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6205560.71</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LassoLars</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6205561.64</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6205562.63</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6212954.45</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LassoLarsCV</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6224992.54</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>LassoCV</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6245111.33</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>LassoLarsIC</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6245114.60</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>RidgeCV</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6252845.39</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>SGDRegressor</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>6266456.70</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuit</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>6339506.90</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuitCV</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>6339506.90</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6438796.80</td>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>RANSACRegressor</th>\n      <td>0.79</td>\n      <td>0.80</td>\n      <td>6647072.24</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>KernelRidge</th>\n      <td>0.78</td>\n      <td>0.79</td>\n      <td>6770052.74</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeRegressor</th>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>7344041.61</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>HuberRegressor</th>\n      <td>0.72</td>\n      <td>0.73</td>\n      <td>7749711.13</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>ElasticNet</th>\n      <td>0.71</td>\n      <td>0.71</td>\n      <td>7901383.93</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.69</td>\n      <td>0.70</td>\n      <td>8073960.79</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>8441281.89</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>TweedieRegressor</th>\n      <td>0.59</td>\n      <td>0.60</td>\n      <td>9361034.29</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>GaussianProcessRegressor</th>\n      <td>0.54</td>\n      <td>0.55</td>\n      <td>9923535.54</td>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveRegressor</th>\n      <td>0.08</td>\n      <td>0.11</td>\n      <td>13971470.22</td>\n      <td>0.17</td>\n    </tr>\n    <tr>\n      <th>ElasticNetCV</th>\n      <td>-0.03</td>\n      <td>-0.00</td>\n      <td>14797869.00</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>BayesianRidge</th>\n      <td>-0.03</td>\n      <td>-0.00</td>\n      <td>14798754.25</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DummyRegressor</th>\n      <td>-0.03</td>\n      <td>-0.00</td>\n      <td>14798754.26</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NuSVR</th>\n      <td>-0.04</td>\n      <td>-0.01</td>\n      <td>14840111.61</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>-0.04</td>\n      <td>-0.01</td>\n      <td>14857857.90</td>\n      <td>0.18</td>\n    </tr>\n    <tr>\n      <th>LinearSVR</th>\n      <td>-0.07</td>\n      <td>-0.04</td>\n      <td>15079955.18</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>-0.07</td>\n      <td>-0.04</td>\n      <td>15080527.14</td>\n      <td>1.10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=[y1_name, y2_name, y3_name], axis=1).values\n",
    "y = df[y2_name].values\n",
    "\n",
    "X, y = shuffle(X, y, random_state=23)\n",
    "\n",
    "# Convert X to float32\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "offset = int(X.shape[0] * 0.8)\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "\n",
    "# LazyRegressor part\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "_, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:22:03.833373965Z",
     "start_time": "2024-01-20T08:21:57.477541075Z"
    }
   },
   "id": "86ee7b7cbe2fe8e9",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 33/42 [00:06<00:02,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileRegressor model failed to execute\n",
      "Solver interior-point is not anymore available in SciPy >= 1.11.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:08<00:00,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2813\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 61820377.021040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               Adjusted R-Squared  R-Squared        RMSE  \\\nModel                                                                      \nLGBMRegressor                                0.70       0.71 16927542.34   \nGradientBoostingRegressor                    0.69       0.70 17131589.06   \nHistGradientBoostingRegressor                0.68       0.69 17404650.90   \nSGDRegressor                                 0.67       0.68 17766591.20   \nRidgeCV                                      0.67       0.68 17835935.21   \nLassoCV                                      0.66       0.67 17894076.63   \nLassoLarsCV                                  0.66       0.67 17896300.47   \nOrthogonalMatchingPursuitCV                  0.66       0.67 17904753.15   \nRidge                                        0.66       0.67 17927491.41   \nLassoLarsIC                                  0.66       0.67 17949531.44   \nLassoLars                                    0.66       0.67 17952192.88   \nLars                                         0.66       0.67 17952199.66   \nLasso                                        0.66       0.67 17952205.52   \nTransformedTargetRegressor                   0.66       0.67 17952208.97   \nLinearRegression                             0.66       0.67 17952208.97   \nLarsCV                                       0.65       0.66 18145045.05   \nRandomForestRegressor                        0.65       0.66 18259563.63   \nExtraTreesRegressor                          0.64       0.65 18487639.41   \nElasticNet                                   0.63       0.64 18688469.85   \nPoissonRegressor                             0.62       0.63 19126954.96   \nXGBRegressor                                 0.61       0.62 19380885.89   \nBaggingRegressor                             0.60       0.61 19460891.45   \nKNeighborsRegressor                          0.59       0.60 19837912.04   \nTweedieRegressor                             0.58       0.59 20042473.16   \nGammaRegressor                               0.55       0.57 20663555.13   \nHuberRegressor                               0.54       0.55 20952115.12   \nRANSACRegressor                              0.53       0.55 21129164.70   \nOrthogonalMatchingPursuit                    0.38       0.40 24272697.93   \nDecisionTreeRegressor                        0.36       0.37 24781332.61   \nExtraTreeRegressor                           0.27       0.29 26459598.69   \nAdaBoostRegressor                            0.26       0.28 26541954.44   \nElasticNetCV                                -0.03      -0.00 31367232.17   \nDummyRegressor                              -0.03      -0.00 31369531.81   \nBayesianRidge                               -0.03      -0.00 31369531.93   \nNuSVR                                       -0.05      -0.02 31628674.51   \nSVR                                         -0.08      -0.05 32049605.30   \nGaussianProcessRegressor                    -0.46      -0.42 37309005.41   \nKernelRidge                                 -3.25      -3.12 63624196.55   \nPassiveAggressiveRegressor                  -3.68      -3.54 66787993.82   \nMLPRegressor                                -3.88      -3.74 68217651.86   \nLinearSVR                                   -3.88      -3.74 68217741.60   \n\n                               Time Taken  \nModel                                      \nLGBMRegressor                        0.08  \nGradientBoostingRegressor            0.52  \nHistGradientBoostingRegressor        0.26  \nSGDRegressor                         0.01  \nRidgeCV                              0.05  \nLassoCV                              0.10  \nLassoLarsCV                          0.05  \nOrthogonalMatchingPursuitCV          0.02  \nRidge                                0.01  \nLassoLarsIC                          0.02  \nLassoLars                            0.02  \nLars                                 0.02  \nLasso                                0.02  \nTransformedTargetRegressor           0.01  \nLinearRegression                     0.02  \nLarsCV                               0.06  \nRandomForestRegressor                1.18  \nExtraTreesRegressor                  0.53  \nElasticNet                           0.02  \nPoissonRegressor                     1.36  \nXGBRegressor                         0.44  \nBaggingRegressor                     0.12  \nKNeighborsRegressor                  0.03  \nTweedieRegressor                     0.64  \nGammaRegressor                       0.38  \nHuberRegressor                       0.03  \nRANSACRegressor                      0.10  \nOrthogonalMatchingPursuit            0.01  \nDecisionTreeRegressor                0.03  \nExtraTreeRegressor                   0.03  \nAdaBoostRegressor                    0.21  \nElasticNetCV                         0.14  \nDummyRegressor                       0.01  \nBayesianRidge                        0.01  \nNuSVR                                0.14  \nSVR                                  0.17  \nGaussianProcessRegressor             0.32  \nKernelRidge                          0.12  \nPassiveAggressiveRegressor           0.16  \nMLPRegressor                         1.27  \nLinearSVR                            0.02  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adjusted R-Squared</th>\n      <th>R-Squared</th>\n      <th>RMSE</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.70</td>\n      <td>0.71</td>\n      <td>16927542.34</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.69</td>\n      <td>0.70</td>\n      <td>17131589.06</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingRegressor</th>\n      <td>0.68</td>\n      <td>0.69</td>\n      <td>17404650.90</td>\n      <td>0.26</td>\n    </tr>\n    <tr>\n      <th>SGDRegressor</th>\n      <td>0.67</td>\n      <td>0.68</td>\n      <td>17766591.20</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>RidgeCV</th>\n      <td>0.67</td>\n      <td>0.68</td>\n      <td>17835935.21</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>LassoCV</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17894076.63</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>LassoLarsCV</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17896300.47</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuitCV</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17904753.15</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17927491.41</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LassoLarsIC</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17949531.44</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LassoLars</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17952192.88</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>Lars</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17952199.66</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17952205.52</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>TransformedTargetRegressor</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17952208.97</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>17952208.97</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LarsCV</th>\n      <td>0.65</td>\n      <td>0.66</td>\n      <td>18145045.05</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.65</td>\n      <td>0.66</td>\n      <td>18259563.63</td>\n      <td>1.18</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.64</td>\n      <td>0.65</td>\n      <td>18487639.41</td>\n      <td>0.53</td>\n    </tr>\n    <tr>\n      <th>ElasticNet</th>\n      <td>0.63</td>\n      <td>0.64</td>\n      <td>18688469.85</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>PoissonRegressor</th>\n      <td>0.62</td>\n      <td>0.63</td>\n      <td>19126954.96</td>\n      <td>1.36</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.61</td>\n      <td>0.62</td>\n      <td>19380885.89</td>\n      <td>0.44</td>\n    </tr>\n    <tr>\n      <th>BaggingRegressor</th>\n      <td>0.60</td>\n      <td>0.61</td>\n      <td>19460891.45</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.59</td>\n      <td>0.60</td>\n      <td>19837912.04</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>TweedieRegressor</th>\n      <td>0.58</td>\n      <td>0.59</td>\n      <td>20042473.16</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>GammaRegressor</th>\n      <td>0.55</td>\n      <td>0.57</td>\n      <td>20663555.13</td>\n      <td>0.38</td>\n    </tr>\n    <tr>\n      <th>HuberRegressor</th>\n      <td>0.54</td>\n      <td>0.55</td>\n      <td>20952115.12</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>RANSACRegressor</th>\n      <td>0.53</td>\n      <td>0.55</td>\n      <td>21129164.70</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuit</th>\n      <td>0.38</td>\n      <td>0.40</td>\n      <td>24272697.93</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.36</td>\n      <td>0.37</td>\n      <td>24781332.61</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeRegressor</th>\n      <td>0.27</td>\n      <td>0.29</td>\n      <td>26459598.69</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.26</td>\n      <td>0.28</td>\n      <td>26541954.44</td>\n      <td>0.21</td>\n    </tr>\n    <tr>\n      <th>ElasticNetCV</th>\n      <td>-0.03</td>\n      <td>-0.00</td>\n      <td>31367232.17</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>DummyRegressor</th>\n      <td>-0.03</td>\n      <td>-0.00</td>\n      <td>31369531.81</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>BayesianRidge</th>\n      <td>-0.03</td>\n      <td>-0.00</td>\n      <td>31369531.93</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NuSVR</th>\n      <td>-0.05</td>\n      <td>-0.02</td>\n      <td>31628674.51</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>-0.08</td>\n      <td>-0.05</td>\n      <td>32049605.30</td>\n      <td>0.17</td>\n    </tr>\n    <tr>\n      <th>GaussianProcessRegressor</th>\n      <td>-0.46</td>\n      <td>-0.42</td>\n      <td>37309005.41</td>\n      <td>0.32</td>\n    </tr>\n    <tr>\n      <th>KernelRidge</th>\n      <td>-3.25</td>\n      <td>-3.12</td>\n      <td>63624196.55</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveRegressor</th>\n      <td>-3.68</td>\n      <td>-3.54</td>\n      <td>66787993.82</td>\n      <td>0.16</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>-3.88</td>\n      <td>-3.74</td>\n      <td>68217651.86</td>\n      <td>1.27</td>\n    </tr>\n    <tr>\n      <th>LinearSVR</th>\n      <td>-3.88</td>\n      <td>-3.74</td>\n      <td>68217741.60</td>\n      <td>0.02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=[y1_name, y2_name, y3_name], axis=1).values\n",
    "y = df[y3_name].values\n",
    "\n",
    "X, y = shuffle(X, y, random_state=25)\n",
    "\n",
    "# Convert X to float32\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "offset = int(X.shape[0] * 0.8)\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "\n",
    "# LazyRegressor part\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:22:58.147823858Z",
     "start_time": "2024-01-20T08:22:49.368008505Z"
    }
   },
   "id": "650986b60dc9004c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from pyGRNN import GRNN\n",
    "\n",
    "df = pd.read_csv('data/dataset.csv')\n",
    "df.head(10)\n",
    "y1_name, y2_name, y3_name = \"dir_costs\",\"traffic_costs_s_r\",\"lost_trips_costs_s_r\" \n",
    "y1, y2, y3 = df[y1_name], df[y2_name], df[y3_name]\n",
    "# 1. scale features\n",
    "X = df.drop(columns=[y1_name, y2_name, y3_name])\n",
    "\n",
    "y = df[y3_name]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessing.minmax_scale(X),\n",
    "                                                    preprocessing.minmax_scale(y.values.reshape((-1, 1))),\n",
    "                                                    test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T09:50:40.157586053Z",
     "start_time": "2024-01-20T09:50:40.114666505Z"
    }
   },
   "id": "12e142ff343533f9",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 390 candidates, totalling 1950 fits\n",
      "Mean Squared Error (MSE):              0.0043720750\n",
      "Root Mean Squared Error (RMSE):        0.0661216683\n",
      "Mean Absolute Error (MAE):             0.0384878099\n",
      "R-squared (R²):                        0.5634845689\n",
      "Mean Absolute Percentage Error (MAPE): 0.4233753439\n",
      "Max Error (ME):                        0.3651697340\n",
      "Median Absolute Error (MedAE):         0.0241684917\n"
     ]
    }
   ],
   "source": [
    "IGRNN = GRNN()\n",
    "params_IGRNN = {'kernel':[\"RBF\"],\n",
    "                'sigma' : list(np.arange(0.1, 4, 0.01)),\n",
    "                'calibration' : ['None']\n",
    "                 }\n",
    "grid_IGRNN = GridSearchCV(estimator=IGRNN,\n",
    "                          param_grid=params_IGRNN,\n",
    "                          scoring='neg_mean_squared_error',\n",
    "                          cv=5,\n",
    "                          verbose=1\n",
    "                          )\n",
    "grid_IGRNN.fit(X_train, y_train.ravel())\n",
    "best_model = grid_IGRNN.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print_metrics(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T09:51:04.704901352Z",
     "start_time": "2024-01-20T09:50:40.444855071Z"
    }
   },
   "id": "37df06e5c76bbad2",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):              0.0037793231\n",
      "Root Mean Squared Error (RMSE):        0.0614761991\n",
      "Mean Absolute Error (MAE):             0.0359004918\n",
      "R-squared (R²):                        0.6226659357\n",
      "Mean Absolute Percentage Error (MAPE): 0.2972523550\n",
      "Max Error (ME):                        0.3573712231\n",
      "Median Absolute Error (MedAE):         0.0189291452\n"
     ]
    }
   ],
   "source": [
    "# Example 2: use Anisotropic GRNN with Limited-Memory BFGS algorithm to select the optimal bandwidths\n",
    "AGRNN = GRNN(calibration=\"gradient_search\")\n",
    "AGRNN.fit(X_train, y_train.ravel())\n",
    "sigma=AGRNN.sigma \n",
    "y_pred = AGRNN.predict(X_test)\n",
    "print_metrics(y_test, y_pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T09:52:31.895906174Z",
     "start_time": "2024-01-20T09:51:04.697786140Z"
    }
   },
   "id": "131bd28eed0fb913",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "744ed43a6079a2bf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
