{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "# read datasets\n",
    "train_df = pd.read_csv('data/train2.csv')\n",
    "test_df = pd.read_csv('data/test2.csv')\n",
    "\n",
    "y1_name, y2_name, y3_name = \"dir_costs\", \"traffic_costs_s_r\", \"lost_trips_costs_s_r\"\n",
    "train_y1, train_y2, train_y3 = train_df[y1_name], train_df[y2_name], train_df[y3_name]\n",
    "test_y1, test_y2, test_y3 = test_df[y1_name], test_df[y2_name], test_df[y3_name]\n",
    "\n",
    "# scale features\n",
    "X_train = train_df.drop(columns=[y1_name, y2_name, y3_name])\n",
    "X_test = test_df.drop(columns=[y1_name, y2_name, y3_name])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:42:48.867709868Z",
     "start_time": "2024-02-08T11:42:47.415709038Z"
    }
   },
   "id": "651cadc1eaadb2ee",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 32/42 [00:15<00:05,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileRegressor model failed to execute\n",
      "Solver interior-point is not anymore available in SciPy >= 1.11.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [01:21<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2815\n",
      "[LightGBM] [Info] Number of data points in the train set: 1290, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 36391114.372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               Adjusted R-Squared  R-Squared        RMSE  \\\nModel                                                                      \nGradientBoostingRegressor                    0.86       0.86  5238623.56   \nRandomForestRegressor                        0.86       0.86  5273191.38   \nExtraTreesRegressor                          0.85       0.86  5302838.83   \nHistGradientBoostingRegressor                0.85       0.85  5433407.17   \nBaggingRegressor                             0.84       0.85  5476407.31   \nLGBMRegressor                                0.84       0.85  5484683.35   \nLasso                                        0.84       0.85  5525846.31   \nLassoLarsCV                                  0.84       0.85  5525846.34   \nTransformedTargetRegressor                   0.84       0.85  5525846.34   \nLinearRegression                             0.84       0.85  5525846.34   \nLassoLars                                    0.84       0.85  5525846.58   \nXGBRegressor                                 0.84       0.85  5528981.98   \nRidge                                        0.84       0.85  5529099.84   \nRidgeCV                                      0.84       0.85  5529099.84   \nLassoLarsIC                                  0.84       0.85  5531741.78   \nLassoCV                                      0.84       0.85  5535650.05   \nPoissonRegressor                             0.84       0.84  5587058.57   \nSGDRegressor                                 0.84       0.84  5591315.32   \nLarsCV                                       0.83       0.84  5710134.71   \nOrthogonalMatchingPursuitCV                  0.83       0.83  5774557.79   \nOrthogonalMatchingPursuit                    0.82       0.83  5908388.19   \nElasticNet                                   0.81       0.82  6040510.41   \nHuberRegressor                               0.81       0.81  6110365.54   \nRANSACRegressor                              0.80       0.81  6203749.07   \nDecisionTreeRegressor                        0.77       0.78  6635701.11   \nTweedieRegressor                             0.77       0.78  6684329.07   \nGammaRegressor                               0.76       0.77  6787540.09   \nKNeighborsRegressor                          0.76       0.77  6794489.77   \nAdaBoostRegressor                            0.69       0.70  7777570.36   \nExtraTreeRegressor                           0.66       0.67  8097951.16   \nLars                                         0.34       0.37 11230469.70   \nElasticNetCV                                -0.04       0.00 14133616.68   \nBayesianRidge                               -0.04      -0.00 14136369.25   \nDummyRegressor                              -0.04      -0.00 14136369.25   \nNuSVR                                       -0.07      -0.03 14370345.64   \nSVR                                         -0.09      -0.05 14466003.52   \nGaussianProcessRegressor                    -0.95      -0.88 19372475.84   \nKernelRidge                                 -6.12      -5.85 37002188.62   \nPassiveAggressiveRegressor                  -6.47      -6.20 37918271.78   \nLinearSVR                                   -6.95      -6.66 39117028.78   \nMLPRegressor                                -6.95      -6.66 39117231.64   \n\n                               Time Taken  \nModel                                      \nGradientBoostingRegressor            0.59  \nRandomForestRegressor                1.60  \nExtraTreesRegressor                  0.44  \nHistGradientBoostingRegressor        6.53  \nBaggingRegressor                     0.12  \nLGBMRegressor                        0.16  \nLasso                                0.12  \nLassoLarsCV                          0.08  \nTransformedTargetRegressor           0.02  \nLinearRegression                     0.03  \nLassoLars                            0.03  \nXGBRegressor                        62.76  \nRidge                                0.02  \nRidgeCV                              0.09  \nLassoLarsIC                          0.04  \nLassoCV                              0.20  \nPoissonRegressor                     2.40  \nSGDRegressor                         0.06  \nLarsCV                               0.07  \nOrthogonalMatchingPursuitCV          0.01  \nOrthogonalMatchingPursuit            0.01  \nElasticNet                           0.01  \nHuberRegressor                       0.12  \nRANSACRegressor                      0.20  \nDecisionTreeRegressor                0.03  \nTweedieRegressor                     1.29  \nGammaRegressor                       0.29  \nKNeighborsRegressor                  0.04  \nAdaBoostRegressor                    0.23  \nExtraTreeRegressor                   0.04  \nLars                                 0.03  \nElasticNetCV                         0.14  \nBayesianRidge                        0.04  \nDummyRegressor                       0.01  \nNuSVR                                0.13  \nSVR                                  0.21  \nGaussianProcessRegressor             0.22  \nKernelRidge                          0.14  \nPassiveAggressiveRegressor           0.08  \nLinearSVR                            0.03  \nMLPRegressor                         3.19  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adjusted R-Squared</th>\n      <th>R-Squared</th>\n      <th>RMSE</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.86</td>\n      <td>0.86</td>\n      <td>5238623.56</td>\n      <td>0.59</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.86</td>\n      <td>0.86</td>\n      <td>5273191.38</td>\n      <td>1.60</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>5302838.83</td>\n      <td>0.44</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingRegressor</th>\n      <td>0.85</td>\n      <td>0.85</td>\n      <td>5433407.17</td>\n      <td>6.53</td>\n    </tr>\n    <tr>\n      <th>BaggingRegressor</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5476407.31</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5484683.35</td>\n      <td>0.16</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5525846.31</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>LassoLarsCV</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5525846.34</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>TransformedTargetRegressor</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5525846.34</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5525846.34</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>LassoLars</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5525846.58</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5528981.98</td>\n      <td>62.76</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5529099.84</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>RidgeCV</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5529099.84</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>LassoLarsIC</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5531741.78</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>LassoCV</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5535650.05</td>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>PoissonRegressor</th>\n      <td>0.84</td>\n      <td>0.84</td>\n      <td>5587058.57</td>\n      <td>2.40</td>\n    </tr>\n    <tr>\n      <th>SGDRegressor</th>\n      <td>0.84</td>\n      <td>0.84</td>\n      <td>5591315.32</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>LarsCV</th>\n      <td>0.83</td>\n      <td>0.84</td>\n      <td>5710134.71</td>\n      <td>0.07</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuitCV</th>\n      <td>0.83</td>\n      <td>0.83</td>\n      <td>5774557.79</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuit</th>\n      <td>0.82</td>\n      <td>0.83</td>\n      <td>5908388.19</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>ElasticNet</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>6040510.41</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>HuberRegressor</th>\n      <td>0.81</td>\n      <td>0.81</td>\n      <td>6110365.54</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>RANSACRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6203749.07</td>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.77</td>\n      <td>0.78</td>\n      <td>6635701.11</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>TweedieRegressor</th>\n      <td>0.77</td>\n      <td>0.78</td>\n      <td>6684329.07</td>\n      <td>1.29</td>\n    </tr>\n    <tr>\n      <th>GammaRegressor</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>6787540.09</td>\n      <td>0.29</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>6794489.77</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.69</td>\n      <td>0.70</td>\n      <td>7777570.36</td>\n      <td>0.23</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeRegressor</th>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>8097951.16</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>Lars</th>\n      <td>0.34</td>\n      <td>0.37</td>\n      <td>11230469.70</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>ElasticNetCV</th>\n      <td>-0.04</td>\n      <td>0.00</td>\n      <td>14133616.68</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>BayesianRidge</th>\n      <td>-0.04</td>\n      <td>-0.00</td>\n      <td>14136369.25</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>DummyRegressor</th>\n      <td>-0.04</td>\n      <td>-0.00</td>\n      <td>14136369.25</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NuSVR</th>\n      <td>-0.07</td>\n      <td>-0.03</td>\n      <td>14370345.64</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>-0.09</td>\n      <td>-0.05</td>\n      <td>14466003.52</td>\n      <td>0.21</td>\n    </tr>\n    <tr>\n      <th>GaussianProcessRegressor</th>\n      <td>-0.95</td>\n      <td>-0.88</td>\n      <td>19372475.84</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>KernelRidge</th>\n      <td>-6.12</td>\n      <td>-5.85</td>\n      <td>37002188.62</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveRegressor</th>\n      <td>-6.47</td>\n      <td>-6.20</td>\n      <td>37918271.78</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>LinearSVR</th>\n      <td>-6.95</td>\n      <td>-6.66</td>\n      <td>39117028.78</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>-6.95</td>\n      <td>-6.66</td>\n      <td>39117231.64</td>\n      <td>3.19</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LazyRegressor part\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = reg.fit(X_train, X_test, train_y1, test_y1)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:44:11.582504544Z",
     "start_time": "2024-02-08T11:42:49.574579432Z"
    }
   },
   "id": "86ee7b7cbe2fe8e9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 10/42 [00:00<00:02, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GammaRegressor model failed to execute\n",
      "Some value(s) of y are out of the valid range of the loss 'HalfGammaLoss'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 32/42 [00:03<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoissonRegressor model failed to execute\n",
      "Some value(s) of y are out of the valid range of the loss 'HalfPoissonLoss'.\n",
      "QuantileRegressor model failed to execute\n",
      "Solver interior-point is not anymore available in SciPy >= 1.11.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:05<00:00,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2815\n",
      "[LightGBM] [Info] Number of data points in the train set: 1290, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 17331737.631923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               Adjusted R-Squared  R-Squared        RMSE  \\\nModel                                                                      \nExtraTreesRegressor                          0.84       0.85 14157210.80   \nXGBRegressor                                 0.83       0.84 14509946.40   \nGradientBoostingRegressor                    0.83       0.84 14523987.50   \nRandomForestRegressor                        0.81       0.82 15321534.02   \nLGBMRegressor                                0.81       0.81 15667018.64   \nBaggingRegressor                             0.81       0.81 15684134.87   \nHistGradientBoostingRegressor                0.80       0.81 15768469.25   \nLarsCV                                       0.68       0.69 20137548.61   \nLassoLarsIC                                  0.68       0.69 20233511.07   \nLassoCV                                      0.68       0.69 20276139.01   \nLassoLarsCV                                  0.68       0.69 20279750.20   \nRidgeCV                                      0.67       0.69 20331589.05   \nRidge                                        0.67       0.69 20344197.75   \nLassoLars                                    0.67       0.69 20345734.40   \nLasso                                        0.67       0.69 20345734.45   \nLinearRegression                             0.67       0.69 20345735.46   \nTransformedTargetRegressor                   0.67       0.69 20345735.46   \nLars                                         0.67       0.69 20345735.46   \nOrthogonalMatchingPursuitCV                  0.67       0.69 20346639.45   \nSGDRegressor                                 0.67       0.69 20352628.82   \nOrthogonalMatchingPursuit                    0.67       0.68 20444970.25   \nKNeighborsRegressor                          0.67       0.68 20538451.70   \nAdaBoostRegressor                            0.62       0.63 21969180.71   \nGaussianProcessRegressor                     0.62       0.63 21993657.74   \nExtraTreeRegressor                           0.61       0.63 22107723.77   \nDecisionTreeRegressor                        0.61       0.62 22243056.95   \nElasticNet                                   0.61       0.62 22325195.91   \nTweedieRegressor                             0.51       0.53 24994132.91   \nRANSACRegressor                              0.48       0.50 25658040.14   \nKernelRidge                                  0.48       0.50 25745882.17   \nHuberRegressor                               0.40       0.42 27565756.49   \nNuSVR                                       -0.04      -0.00 36313479.98   \nSVR                                         -0.04      -0.00 36336846.10   \nElasticNetCV                                -0.06      -0.02 36682107.08   \nBayesianRidge                               -0.06      -0.02 36682961.09   \nDummyRegressor                              -0.06      -0.02 36682961.09   \nPassiveAggressiveRegressor                  -0.10      -0.06 37365573.19   \nLinearSVR                                   -0.15      -0.11 38251486.99   \nMLPRegressor                                -0.15      -0.11 38251983.14   \n\n                               Time Taken  \nModel                                      \nExtraTreesRegressor                  0.39  \nXGBRegressor                         0.21  \nGradientBoostingRegressor            0.46  \nRandomForestRegressor                0.86  \nLGBMRegressor                        0.06  \nBaggingRegressor                     0.10  \nHistGradientBoostingRegressor        0.22  \nLarsCV                               0.05  \nLassoLarsIC                          0.03  \nLassoCV                              0.13  \nLassoLarsCV                          0.05  \nRidgeCV                              0.01  \nRidge                                0.01  \nLassoLars                            0.02  \nLasso                                0.05  \nLinearRegression                     0.01  \nTransformedTargetRegressor           0.01  \nLars                                 0.02  \nOrthogonalMatchingPursuitCV          0.02  \nSGDRegressor                         0.01  \nOrthogonalMatchingPursuit            0.01  \nKNeighborsRegressor                  0.02  \nAdaBoostRegressor                    0.19  \nGaussianProcessRegressor             0.15  \nExtraTreeRegressor                   0.01  \nDecisionTreeRegressor                0.02  \nElasticNet                           0.01  \nTweedieRegressor                     0.65  \nRANSACRegressor                      0.09  \nKernelRidge                          0.08  \nHuberRegressor                       0.01  \nNuSVR                                0.10  \nSVR                                  0.13  \nElasticNetCV                         0.08  \nBayesianRidge                        0.01  \nDummyRegressor                       0.01  \nPassiveAggressiveRegressor           0.08  \nLinearSVR                            0.01  \nMLPRegressor                         0.86  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adjusted R-Squared</th>\n      <th>R-Squared</th>\n      <th>RMSE</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>14157210.80</td>\n      <td>0.39</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.83</td>\n      <td>0.84</td>\n      <td>14509946.40</td>\n      <td>0.21</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.83</td>\n      <td>0.84</td>\n      <td>14523987.50</td>\n      <td>0.46</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>15321534.02</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.81</td>\n      <td>0.81</td>\n      <td>15667018.64</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>BaggingRegressor</th>\n      <td>0.81</td>\n      <td>0.81</td>\n      <td>15684134.87</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>15768469.25</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>LarsCV</th>\n      <td>0.68</td>\n      <td>0.69</td>\n      <td>20137548.61</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>LassoLarsIC</th>\n      <td>0.68</td>\n      <td>0.69</td>\n      <td>20233511.07</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>LassoCV</th>\n      <td>0.68</td>\n      <td>0.69</td>\n      <td>20276139.01</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>LassoLarsCV</th>\n      <td>0.68</td>\n      <td>0.69</td>\n      <td>20279750.20</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>RidgeCV</th>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>20331589.05</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>20344197.75</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LassoLars</th>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>20345734.40</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>20345734.45</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>20345735.46</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>TransformedTargetRegressor</th>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>20345735.46</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>Lars</th>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>20345735.46</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuitCV</th>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>20346639.45</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>SGDRegressor</th>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>20352628.82</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuit</th>\n      <td>0.67</td>\n      <td>0.68</td>\n      <td>20444970.25</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.67</td>\n      <td>0.68</td>\n      <td>20538451.70</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.62</td>\n      <td>0.63</td>\n      <td>21969180.71</td>\n      <td>0.19</td>\n    </tr>\n    <tr>\n      <th>GaussianProcessRegressor</th>\n      <td>0.62</td>\n      <td>0.63</td>\n      <td>21993657.74</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeRegressor</th>\n      <td>0.61</td>\n      <td>0.63</td>\n      <td>22107723.77</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.61</td>\n      <td>0.62</td>\n      <td>22243056.95</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>ElasticNet</th>\n      <td>0.61</td>\n      <td>0.62</td>\n      <td>22325195.91</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>TweedieRegressor</th>\n      <td>0.51</td>\n      <td>0.53</td>\n      <td>24994132.91</td>\n      <td>0.65</td>\n    </tr>\n    <tr>\n      <th>RANSACRegressor</th>\n      <td>0.48</td>\n      <td>0.50</td>\n      <td>25658040.14</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>KernelRidge</th>\n      <td>0.48</td>\n      <td>0.50</td>\n      <td>25745882.17</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>HuberRegressor</th>\n      <td>0.40</td>\n      <td>0.42</td>\n      <td>27565756.49</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NuSVR</th>\n      <td>-0.04</td>\n      <td>-0.00</td>\n      <td>36313479.98</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>-0.04</td>\n      <td>-0.00</td>\n      <td>36336846.10</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>ElasticNetCV</th>\n      <td>-0.06</td>\n      <td>-0.02</td>\n      <td>36682107.08</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>BayesianRidge</th>\n      <td>-0.06</td>\n      <td>-0.02</td>\n      <td>36682961.09</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DummyRegressor</th>\n      <td>-0.06</td>\n      <td>-0.02</td>\n      <td>36682961.09</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveRegressor</th>\n      <td>-0.10</td>\n      <td>-0.06</td>\n      <td>37365573.19</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>LinearSVR</th>\n      <td>-0.15</td>\n      <td>-0.11</td>\n      <td>38251486.99</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>-0.15</td>\n      <td>-0.11</td>\n      <td>38251983.14</td>\n      <td>0.86</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LazyRegressor part\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = reg.fit(X_train, X_test, train_y2, test_y2)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:47:03.074043597Z",
     "start_time": "2024-02-08T11:46:57.779919933Z"
    }
   },
   "id": "650986b60dc9004c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 33/42 [00:05<00:01,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileRegressor model failed to execute\n",
      "Solver interior-point is not anymore available in SciPy >= 1.11.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:07<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2815\n",
      "[LightGBM] [Info] Number of data points in the train set: 1290, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 77614877.925581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               Adjusted R-Squared  R-Squared        RMSE  \\\nModel                                                                      \nHistGradientBoostingRegressor                0.68       0.69 28037543.95   \nGradientBoostingRegressor                    0.68       0.69 28070875.80   \nLGBMRegressor                                0.67       0.68 28246802.47   \nRandomForestRegressor                        0.64       0.66 29358090.21   \nExtraTreesRegressor                          0.63       0.65 29788765.81   \nXGBRegressor                                 0.62       0.63 30397907.17   \nPoissonRegressor                             0.60       0.62 31087633.40   \nBaggingRegressor                             0.60       0.61 31195186.65   \nLasso                                        0.54       0.56 33324749.57   \nLassoLars                                    0.54       0.56 33324753.20   \nTransformedTargetRegressor                   0.54       0.56 33324753.30   \nLinearRegression                             0.54       0.56 33324753.30   \nLassoLarsIC                                  0.54       0.56 33329493.63   \nRidge                                        0.54       0.56 33342077.22   \nRidgeCV                                      0.54       0.56 33342077.22   \nLassoCV                                      0.54       0.56 33483199.87   \nLassoLarsCV                                  0.54       0.56 33485663.34   \nOrthogonalMatchingPursuitCV                  0.54       0.55 33498308.28   \nSGDRegressor                                 0.54       0.55 33538879.36   \nLarsCV                                       0.51       0.53 34457712.59   \nElasticNet                                   0.49       0.51 35110620.12   \nKNeighborsRegressor                          0.45       0.47 36462500.57   \nTweedieRegressor                             0.43       0.45 37331294.29   \nGammaRegressor                               0.40       0.42 38165036.87   \nExtraTreeRegressor                           0.37       0.39 39234242.16   \nRANSACRegressor                              0.33       0.36 40198577.12   \nAdaBoostRegressor                            0.30       0.32 41320749.98   \nDecisionTreeRegressor                        0.29       0.32 41456416.64   \nHuberRegressor                               0.21       0.24 43857467.76   \nOrthogonalMatchingPursuit                    0.17       0.20 44968833.51   \nGaussianProcessRegressor                     0.00       0.04 49207498.76   \nElasticNetCV                                -0.04      -0.00 50205620.78   \nBayesianRidge                               -0.04      -0.00 50207209.16   \nDummyRegressor                              -0.04      -0.00 50207209.16   \nNuSVR                                       -0.07      -0.03 51000441.05   \nSVR                                         -0.12      -0.08 52075559.28   \nLars                                        -0.80      -0.73 66049712.56   \nKernelRidge                                 -2.11      -2.00 86925853.98   \nPassiveAggressiveRegressor                  -2.41      -2.28 90901094.75   \nLinearSVR                                   -2.49      -2.36 91978179.85   \nMLPRegressor                                -2.49      -2.36 91978364.73   \n\n                               Time Taken  \nModel                                      \nHistGradientBoostingRegressor        0.19  \nGradientBoostingRegressor            0.48  \nLGBMRegressor                        0.06  \nRandomForestRegressor                0.89  \nExtraTreesRegressor                  0.42  \nXGBRegressor                         0.22  \nPoissonRegressor                     1.28  \nBaggingRegressor                     0.10  \nLasso                                0.02  \nLassoLars                            0.02  \nTransformedTargetRegressor           0.01  \nLinearRegression                     0.02  \nLassoLarsIC                          0.02  \nRidge                                0.01  \nRidgeCV                              0.02  \nLassoCV                              0.16  \nLassoLarsCV                          0.05  \nOrthogonalMatchingPursuitCV          0.01  \nSGDRegressor                         0.01  \nLarsCV                               0.04  \nElasticNet                           0.03  \nKNeighborsRegressor                  0.02  \nTweedieRegressor                     0.65  \nGammaRegressor                       0.27  \nExtraTreeRegressor                   0.02  \nRANSACRegressor                      0.12  \nAdaBoostRegressor                    0.19  \nDecisionTreeRegressor                0.04  \nHuberRegressor                       0.02  \nOrthogonalMatchingPursuit            0.01  \nGaussianProcessRegressor             0.19  \nElasticNetCV                         0.14  \nBayesianRidge                        0.01  \nDummyRegressor                       0.02  \nNuSVR                                0.11  \nSVR                                  0.13  \nLars                                 0.02  \nKernelRidge                          0.07  \nPassiveAggressiveRegressor           0.07  \nLinearSVR                            0.01  \nMLPRegressor                         0.88  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adjusted R-Squared</th>\n      <th>R-Squared</th>\n      <th>RMSE</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>HistGradientBoostingRegressor</th>\n      <td>0.68</td>\n      <td>0.69</td>\n      <td>28037543.95</td>\n      <td>0.19</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.68</td>\n      <td>0.69</td>\n      <td>28070875.80</td>\n      <td>0.48</td>\n    </tr>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.67</td>\n      <td>0.68</td>\n      <td>28246802.47</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.64</td>\n      <td>0.66</td>\n      <td>29358090.21</td>\n      <td>0.89</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.63</td>\n      <td>0.65</td>\n      <td>29788765.81</td>\n      <td>0.42</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.62</td>\n      <td>0.63</td>\n      <td>30397907.17</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>PoissonRegressor</th>\n      <td>0.60</td>\n      <td>0.62</td>\n      <td>31087633.40</td>\n      <td>1.28</td>\n    </tr>\n    <tr>\n      <th>BaggingRegressor</th>\n      <td>0.60</td>\n      <td>0.61</td>\n      <td>31195186.65</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.54</td>\n      <td>0.56</td>\n      <td>33324749.57</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LassoLars</th>\n      <td>0.54</td>\n      <td>0.56</td>\n      <td>33324753.20</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>TransformedTargetRegressor</th>\n      <td>0.54</td>\n      <td>0.56</td>\n      <td>33324753.30</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.54</td>\n      <td>0.56</td>\n      <td>33324753.30</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LassoLarsIC</th>\n      <td>0.54</td>\n      <td>0.56</td>\n      <td>33329493.63</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.54</td>\n      <td>0.56</td>\n      <td>33342077.22</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>RidgeCV</th>\n      <td>0.54</td>\n      <td>0.56</td>\n      <td>33342077.22</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LassoCV</th>\n      <td>0.54</td>\n      <td>0.56</td>\n      <td>33483199.87</td>\n      <td>0.16</td>\n    </tr>\n    <tr>\n      <th>LassoLarsCV</th>\n      <td>0.54</td>\n      <td>0.56</td>\n      <td>33485663.34</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuitCV</th>\n      <td>0.54</td>\n      <td>0.55</td>\n      <td>33498308.28</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>SGDRegressor</th>\n      <td>0.54</td>\n      <td>0.55</td>\n      <td>33538879.36</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LarsCV</th>\n      <td>0.51</td>\n      <td>0.53</td>\n      <td>34457712.59</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>ElasticNet</th>\n      <td>0.49</td>\n      <td>0.51</td>\n      <td>35110620.12</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.45</td>\n      <td>0.47</td>\n      <td>36462500.57</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>TweedieRegressor</th>\n      <td>0.43</td>\n      <td>0.45</td>\n      <td>37331294.29</td>\n      <td>0.65</td>\n    </tr>\n    <tr>\n      <th>GammaRegressor</th>\n      <td>0.40</td>\n      <td>0.42</td>\n      <td>38165036.87</td>\n      <td>0.27</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeRegressor</th>\n      <td>0.37</td>\n      <td>0.39</td>\n      <td>39234242.16</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>RANSACRegressor</th>\n      <td>0.33</td>\n      <td>0.36</td>\n      <td>40198577.12</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.30</td>\n      <td>0.32</td>\n      <td>41320749.98</td>\n      <td>0.19</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.29</td>\n      <td>0.32</td>\n      <td>41456416.64</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>HuberRegressor</th>\n      <td>0.21</td>\n      <td>0.24</td>\n      <td>43857467.76</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuit</th>\n      <td>0.17</td>\n      <td>0.20</td>\n      <td>44968833.51</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>GaussianProcessRegressor</th>\n      <td>0.00</td>\n      <td>0.04</td>\n      <td>49207498.76</td>\n      <td>0.19</td>\n    </tr>\n    <tr>\n      <th>ElasticNetCV</th>\n      <td>-0.04</td>\n      <td>-0.00</td>\n      <td>50205620.78</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>BayesianRidge</th>\n      <td>-0.04</td>\n      <td>-0.00</td>\n      <td>50207209.16</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DummyRegressor</th>\n      <td>-0.04</td>\n      <td>-0.00</td>\n      <td>50207209.16</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>NuSVR</th>\n      <td>-0.07</td>\n      <td>-0.03</td>\n      <td>51000441.05</td>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>-0.12</td>\n      <td>-0.08</td>\n      <td>52075559.28</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>Lars</th>\n      <td>-0.80</td>\n      <td>-0.73</td>\n      <td>66049712.56</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>KernelRidge</th>\n      <td>-2.11</td>\n      <td>-2.00</td>\n      <td>86925853.98</td>\n      <td>0.07</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveRegressor</th>\n      <td>-2.41</td>\n      <td>-2.28</td>\n      <td>90901094.75</td>\n      <td>0.07</td>\n    </tr>\n    <tr>\n      <th>LinearSVR</th>\n      <td>-2.49</td>\n      <td>-2.36</td>\n      <td>91978179.85</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>-2.49</td>\n      <td>-2.36</td>\n      <td>91978364.73</td>\n      <td>0.88</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LazyRegressor part\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = reg.fit(X_train, X_test, train_y3, test_y3)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:47:18.034945129Z",
     "start_time": "2024-02-08T11:47:10.913694549Z"
    }
   },
   "id": "12e142ff343533f9",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8ba85fc75c8fa019"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
