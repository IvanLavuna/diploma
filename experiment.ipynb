{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 1. Import needed libraries <h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79688f1d557517c2"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:08:37.394777724Z",
     "start_time": "2024-01-20T08:08:37.354242549Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from pyGRNN import GRNN\n",
    "from sklearn.cluster import KMeans\n",
    "from lazypredict.Supervised import LazyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from numpy import sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, max_error, median_absolute_error, mean_absolute_error\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(f'Mean Squared Error (MSE):              {mse:.10f}')\n",
    "    \n",
    "    rmse = sqrt(mse)\n",
    "    print(f'Root Mean Squared Error (RMSE):        {rmse:.10f}')\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f'Mean Absolute Error (MAE):             {mae:.10f}')\n",
    "    \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f'R-squared (R²):                        {r2:.10f}')\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    print(f'Mean Absolute Percentage Error (MAPE): {mape:.10f}')    \n",
    "    \n",
    "    me = max_error(y_true, y_pred)\n",
    "    print(f'Max Error (ME):                        {me:.10f}')    \n",
    "    \n",
    "    med_ae = median_absolute_error(y_true, y_pred)\n",
    "    print(f'Median Absolute Error (MedAE):         {med_ae:.10f}') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:17:41.802109380Z",
     "start_time": "2024-01-20T08:17:41.760838319Z"
    }
   },
   "id": "20fffa5e7f462c88",
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 2. Read data </h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24b006c3b897c872"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   max_discharge_sum_r1  max_discharge_sum_r2_1  max_discharge_sum_r2_2  \\\n0                585.67                 1626.36                 1917.92   \n1                577.56                 1643.06                 1887.66   \n2                772.58                 1672.02                 1803.84   \n3                522.08                 1653.12                 3326.49   \n4                614.14                 1644.18                 2485.55   \n5                813.29                 1669.06                 1877.53   \n6                663.21                 1666.04                 2230.32   \n7                501.75                 1726.18                 2023.48   \n8                581.13                 1669.38                 1976.99   \n9                622.46                 1636.29                 2176.80   \n\n   max_discharge_sum_r3  flood_sum_r1  flood_sum_r2_1  flood_sum_r2_2  \\\n0                638.86       8411.14        31506.11        22090.55   \n1                821.35       8428.71        31464.77        21934.20   \n2                494.53       9338.14        31334.39        21498.21   \n3                460.21       8055.62        40539.30        32937.77   \n4                595.06       8529.56        34186.48        28422.80   \n5                680.46       9701.22        31193.17        22297.85   \n6                671.19       8806.26        32504.62        26343.62   \n7                993.98       8031.13        32844.96        22531.50   \n8               1105.23       8297.44        32213.13        22180.60   \n9                457.47       8532.34        32557.58        24402.16   \n\n   flood_sum_r3  HW_inund_uncertainty  bridge_scour_uncertainty  \\\n0      15338.74                  1.03                      1.14   \n1      17058.63                  1.03                      0.91   \n2      13903.41                  1.08                      0.95   \n3      13073.49                  1.05                      1.26   \n4      14950.68                  0.79                      0.70   \n5      15652.70                  0.78                      1.38   \n6      15618.30                  0.88                      0.74   \n7      18555.23                  0.64                      0.35   \n8      19413.06                  0.97                      0.52   \n9      13253.84                  0.86                      1.26   \n\n   travel_demand_uncertainty  restoration_capacity_uncertainty   dir_costs  \\\n0                       0.76                                 6 33407655.48   \n1                       1.14                                 5 26222060.06   \n2                       1.06                                 3 25265321.95   \n3                       0.98                                 3 54860001.97   \n4                       1.23                                 4 39212152.98   \n5                       0.88                                 6 32258229.36   \n6                       0.64                                 6 35019441.00   \n7                       1.19                                 5 28815518.56   \n8                       0.94                                 6 31828474.78   \n9                       1.16                                 4 45656687.89   \n\n   lost_trips_costs_s_r  traffic_costs_s_r  \n0           63906033.49       -13876077.42  \n1           55359791.46         9384497.19  \n2           59332912.51         5139630.61  \n3           94212106.15         2127730.92  \n4           76321454.19        21132083.35  \n5           76162964.60        -6506723.41  \n6           32830011.84       -17554044.81  \n7           71389907.84        17423834.01  \n8           56818034.31        -2533088.91  \n9           61532735.19        14043780.74  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>max_discharge_sum_r1</th>\n      <th>max_discharge_sum_r2_1</th>\n      <th>max_discharge_sum_r2_2</th>\n      <th>max_discharge_sum_r3</th>\n      <th>flood_sum_r1</th>\n      <th>flood_sum_r2_1</th>\n      <th>flood_sum_r2_2</th>\n      <th>flood_sum_r3</th>\n      <th>HW_inund_uncertainty</th>\n      <th>bridge_scour_uncertainty</th>\n      <th>travel_demand_uncertainty</th>\n      <th>restoration_capacity_uncertainty</th>\n      <th>dir_costs</th>\n      <th>lost_trips_costs_s_r</th>\n      <th>traffic_costs_s_r</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>585.67</td>\n      <td>1626.36</td>\n      <td>1917.92</td>\n      <td>638.86</td>\n      <td>8411.14</td>\n      <td>31506.11</td>\n      <td>22090.55</td>\n      <td>15338.74</td>\n      <td>1.03</td>\n      <td>1.14</td>\n      <td>0.76</td>\n      <td>6</td>\n      <td>33407655.48</td>\n      <td>63906033.49</td>\n      <td>-13876077.42</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>577.56</td>\n      <td>1643.06</td>\n      <td>1887.66</td>\n      <td>821.35</td>\n      <td>8428.71</td>\n      <td>31464.77</td>\n      <td>21934.20</td>\n      <td>17058.63</td>\n      <td>1.03</td>\n      <td>0.91</td>\n      <td>1.14</td>\n      <td>5</td>\n      <td>26222060.06</td>\n      <td>55359791.46</td>\n      <td>9384497.19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>772.58</td>\n      <td>1672.02</td>\n      <td>1803.84</td>\n      <td>494.53</td>\n      <td>9338.14</td>\n      <td>31334.39</td>\n      <td>21498.21</td>\n      <td>13903.41</td>\n      <td>1.08</td>\n      <td>0.95</td>\n      <td>1.06</td>\n      <td>3</td>\n      <td>25265321.95</td>\n      <td>59332912.51</td>\n      <td>5139630.61</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>522.08</td>\n      <td>1653.12</td>\n      <td>3326.49</td>\n      <td>460.21</td>\n      <td>8055.62</td>\n      <td>40539.30</td>\n      <td>32937.77</td>\n      <td>13073.49</td>\n      <td>1.05</td>\n      <td>1.26</td>\n      <td>0.98</td>\n      <td>3</td>\n      <td>54860001.97</td>\n      <td>94212106.15</td>\n      <td>2127730.92</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>614.14</td>\n      <td>1644.18</td>\n      <td>2485.55</td>\n      <td>595.06</td>\n      <td>8529.56</td>\n      <td>34186.48</td>\n      <td>28422.80</td>\n      <td>14950.68</td>\n      <td>0.79</td>\n      <td>0.70</td>\n      <td>1.23</td>\n      <td>4</td>\n      <td>39212152.98</td>\n      <td>76321454.19</td>\n      <td>21132083.35</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>813.29</td>\n      <td>1669.06</td>\n      <td>1877.53</td>\n      <td>680.46</td>\n      <td>9701.22</td>\n      <td>31193.17</td>\n      <td>22297.85</td>\n      <td>15652.70</td>\n      <td>0.78</td>\n      <td>1.38</td>\n      <td>0.88</td>\n      <td>6</td>\n      <td>32258229.36</td>\n      <td>76162964.60</td>\n      <td>-6506723.41</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>663.21</td>\n      <td>1666.04</td>\n      <td>2230.32</td>\n      <td>671.19</td>\n      <td>8806.26</td>\n      <td>32504.62</td>\n      <td>26343.62</td>\n      <td>15618.30</td>\n      <td>0.88</td>\n      <td>0.74</td>\n      <td>0.64</td>\n      <td>6</td>\n      <td>35019441.00</td>\n      <td>32830011.84</td>\n      <td>-17554044.81</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>501.75</td>\n      <td>1726.18</td>\n      <td>2023.48</td>\n      <td>993.98</td>\n      <td>8031.13</td>\n      <td>32844.96</td>\n      <td>22531.50</td>\n      <td>18555.23</td>\n      <td>0.64</td>\n      <td>0.35</td>\n      <td>1.19</td>\n      <td>5</td>\n      <td>28815518.56</td>\n      <td>71389907.84</td>\n      <td>17423834.01</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>581.13</td>\n      <td>1669.38</td>\n      <td>1976.99</td>\n      <td>1105.23</td>\n      <td>8297.44</td>\n      <td>32213.13</td>\n      <td>22180.60</td>\n      <td>19413.06</td>\n      <td>0.97</td>\n      <td>0.52</td>\n      <td>0.94</td>\n      <td>6</td>\n      <td>31828474.78</td>\n      <td>56818034.31</td>\n      <td>-2533088.91</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>622.46</td>\n      <td>1636.29</td>\n      <td>2176.80</td>\n      <td>457.47</td>\n      <td>8532.34</td>\n      <td>32557.58</td>\n      <td>24402.16</td>\n      <td>13253.84</td>\n      <td>0.86</td>\n      <td>1.26</td>\n      <td>1.16</td>\n      <td>4</td>\n      <td>45656687.89</td>\n      <td>61532735.19</td>\n      <td>14043780.74</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:17:42.455541465Z",
     "start_time": "2024-01-20T08:17:42.437187351Z"
    }
   },
   "id": "9afe8e21c6be336b",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:17:44.597315995Z",
     "start_time": "2024-01-20T08:17:44.558436173Z"
    }
   },
   "id": "4442f993a9f35c88",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y1_name, y2_name, y3_name = \"dir_costs\",\"traffic_costs_s_r\",\"lost_trips_costs_s_r\" \n",
    "y1, y2, y3 = df[y1_name], df[y2_name], df[y3_name]\n",
    "test_y1, test_y2, test_y3 = test_df[y1_name], test_df[y2_name], test_df[y3_name]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:17:44.994575756Z",
     "start_time": "2024-01-20T08:17:44.970571504Z"
    }
   },
   "id": "db318421c7424316",
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 3. Scale features </h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "153c8729a4e901a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = df.drop(columns=[y1_name, y2_name, y3_name])\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:17:45.685228688Z",
     "start_time": "2024-01-20T08:17:45.673273115Z"
    }
   },
   "id": "ad600d6f773b8a2b",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_X = test_df.drop(columns=[y1_name, y2_name, y3_name])\n",
    "X_test_scaled = scaler.transform(test_X)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=test_X.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:17:53.662112777Z",
     "start_time": "2024-01-20T08:17:53.637063701Z"
    }
   },
   "id": "a36716fa43ae7a71",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2813\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 2376948.061796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               Adjusted R-Squared  R-Squared        RMSE  \\\nModel                                                                      \nXGBRegressor                                 0.84       0.84  6061095.29   \nLGBMRegressor                                0.83       0.84  6094770.44   \nExtraTreesRegressor                          0.82       0.82  6382617.83   \nHistGradientBoostingRegressor                0.82       0.82  6405705.96   \nBaggingRegressor                             0.82       0.82  6424554.95   \nRandomForestRegressor                        0.81       0.82  6441033.11   \nGradientBoostingRegressor                    0.81       0.82  6521976.44   \nLars                                         0.77       0.77  7240474.22   \nLassoLars                                    0.77       0.77  7241961.89   \nTransformedTargetRegressor                   0.77       0.77  7241961.89   \nLinearRegression                             0.77       0.77  7241961.89   \nLasso                                        0.77       0.77  7241961.94   \nLassoLarsCV                                  0.77       0.77  7242131.94   \nRidge                                        0.77       0.77  7242401.35   \nLassoLarsIC                                  0.77       0.77  7243056.56   \nLarsCV                                       0.77       0.77  7243412.18   \nRidgeCV                                      0.76       0.77  7250178.25   \nLassoCV                                      0.76       0.77  7251181.30   \nSGDRegressor                                 0.76       0.77  7258232.01   \nOrthogonalMatchingPursuitCV                  0.76       0.77  7283642.93   \nOrthogonalMatchingPursuit                    0.76       0.77  7333705.69   \nRANSACRegressor                              0.75       0.76  7455186.22   \nKernelRidge                                  0.74       0.75  7589203.21   \nDecisionTreeRegressor                        0.74       0.75  7620937.08   \nHuberRegressor                               0.69       0.69  8385748.07   \nElasticNet                                   0.67       0.68  8558608.68   \nAdaBoostRegressor                            0.67       0.68  8639749.93   \nKNeighborsRegressor                          0.64       0.65  8956843.41   \nExtraTreeRegressor                           0.61       0.63  9283700.86   \nTweedieRegressor                             0.56       0.57  9906860.20   \nGaussianProcessRegressor                     0.52       0.53 10380507.53   \nPassiveAggressiveRegressor                   0.07       0.10 14374761.87   \nElasticNetCV                                -0.03      -0.00 15187800.42   \nBayesianRidge                               -0.03      -0.00 15188667.82   \nDummyRegressor                              -0.03      -0.00 15188667.82   \nNuSVR                                       -0.04      -0.01 15237754.90   \nSVR                                         -0.05      -0.01 15278051.82   \nLinearSVR                                   -0.07      -0.04 15484571.08   \nMLPRegressor                                -0.07      -0.04 15485164.50   \n\n                               Time Taken  \nModel                                      \nXGBRegressor                         0.71  \nLGBMRegressor                        0.06  \nExtraTreesRegressor                  0.57  \nHistGradientBoostingRegressor        0.32  \nBaggingRegressor                     0.12  \nRandomForestRegressor                1.12  \nGradientBoostingRegressor            0.62  \nLars                                 0.02  \nLassoLars                            0.02  \nTransformedTargetRegressor           0.01  \nLinearRegression                     0.01  \nLasso                                0.02  \nLassoLarsCV                          0.06  \nRidge                                0.01  \nLassoLarsIC                          0.01  \nLarsCV                               0.03  \nRidgeCV                              0.04  \nLassoCV                              0.09  \nSGDRegressor                         0.01  \nOrthogonalMatchingPursuitCV          0.01  \nOrthogonalMatchingPursuit            0.01  \nRANSACRegressor                      0.04  \nKernelRidge                          0.09  \nDecisionTreeRegressor                0.03  \nHuberRegressor                       0.02  \nElasticNet                           0.03  \nAdaBoostRegressor                    0.17  \nKNeighborsRegressor                  0.02  \nExtraTreeRegressor                   0.03  \nTweedieRegressor                     0.72  \nGaussianProcessRegressor             0.20  \nPassiveAggressiveRegressor           0.10  \nElasticNetCV                         0.10  \nBayesianRidge                        0.01  \nDummyRegressor                       0.01  \nNuSVR                                0.15  \nSVR                                  0.16  \nLinearSVR                            0.01  \nMLPRegressor                         1.10  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adjusted R-Squared</th>\n      <th>R-Squared</th>\n      <th>RMSE</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.84</td>\n      <td>0.84</td>\n      <td>6061095.29</td>\n      <td>0.71</td>\n    </tr>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.83</td>\n      <td>0.84</td>\n      <td>6094770.44</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6382617.83</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingRegressor</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6405705.96</td>\n      <td>0.32</td>\n    </tr>\n    <tr>\n      <th>BaggingRegressor</th>\n      <td>0.82</td>\n      <td>0.82</td>\n      <td>6424554.95</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>6441033.11</td>\n      <td>1.12</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>6521976.44</td>\n      <td>0.62</td>\n    </tr>\n    <tr>\n      <th>Lars</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7240474.22</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LassoLars</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7241961.89</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>TransformedTargetRegressor</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7241961.89</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7241961.89</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7241961.94</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LassoLarsCV</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7242131.94</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7242401.35</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LassoLarsIC</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7243056.56</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LarsCV</th>\n      <td>0.77</td>\n      <td>0.77</td>\n      <td>7243412.18</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>RidgeCV</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>7250178.25</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>LassoCV</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>7251181.30</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>SGDRegressor</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>7258232.01</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuitCV</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>7283642.93</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuit</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>7333705.69</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>RANSACRegressor</th>\n      <td>0.75</td>\n      <td>0.76</td>\n      <td>7455186.22</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>KernelRidge</th>\n      <td>0.74</td>\n      <td>0.75</td>\n      <td>7589203.21</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.74</td>\n      <td>0.75</td>\n      <td>7620937.08</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>HuberRegressor</th>\n      <td>0.69</td>\n      <td>0.69</td>\n      <td>8385748.07</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>ElasticNet</th>\n      <td>0.67</td>\n      <td>0.68</td>\n      <td>8558608.68</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.67</td>\n      <td>0.68</td>\n      <td>8639749.93</td>\n      <td>0.17</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.64</td>\n      <td>0.65</td>\n      <td>8956843.41</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeRegressor</th>\n      <td>0.61</td>\n      <td>0.63</td>\n      <td>9283700.86</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>TweedieRegressor</th>\n      <td>0.56</td>\n      <td>0.57</td>\n      <td>9906860.20</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>GaussianProcessRegressor</th>\n      <td>0.52</td>\n      <td>0.53</td>\n      <td>10380507.53</td>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveRegressor</th>\n      <td>0.07</td>\n      <td>0.10</td>\n      <td>14374761.87</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>ElasticNetCV</th>\n      <td>-0.03</td>\n      <td>-0.00</td>\n      <td>15187800.42</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>BayesianRidge</th>\n      <td>-0.03</td>\n      <td>-0.00</td>\n      <td>15188667.82</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DummyRegressor</th>\n      <td>-0.03</td>\n      <td>-0.00</td>\n      <td>15188667.82</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NuSVR</th>\n      <td>-0.04</td>\n      <td>-0.01</td>\n      <td>15237754.90</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>-0.05</td>\n      <td>-0.01</td>\n      <td>15278051.82</td>\n      <td>0.16</td>\n    </tr>\n    <tr>\n      <th>LinearSVR</th>\n      <td>-0.07</td>\n      <td>-0.04</td>\n      <td>15484571.08</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>-0.07</td>\n      <td>-0.04</td>\n      <td>15485164.50</td>\n      <td>1.10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2 = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "_, predictions2 = reg2.fit(X, test_X, y2, test_y2)\n",
    "predictions2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:19:17.549097373Z",
     "start_time": "2024-01-20T08:19:10.614406032Z"
    }
   },
   "id": "7de5562885e1a847",
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p> <b> Міні-висновок: </b> Проблема не в масштабуванні даних. Масшатбування даних трохи покращує RMSE 5461130.73 -> 4821200.67 </p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24c2d8ee42fd237c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 4. Add clusters </h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fd3c8c2f428265d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4> a. train </h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dc88850f43e1936"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters =  10\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "print(\"number of clusters = \", k)\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(X_scaled_df)\n",
    "labels = kmeans.labels_\n",
    "# Add the cluster labels to the train DataFrame\n",
    "X_scaled_clustered_df = X_scaled_df.copy(deep=True)\n",
    "X_scaled_clustered_df['Cluster'] = labels\n",
    "dummy_columns = pd.get_dummies(X_scaled_clustered_df['Cluster'], prefix='class')\n",
    "X_scaled_clustered_df = pd.concat([X_scaled_clustered_df, dummy_columns], axis=1)\n",
    "X_scaled_clustered_df = X_scaled_clustered_df.drop(\"Cluster\", axis=1)\n",
    "\n",
    "# Add the cluster labels to the test DataFrame\n",
    "test_clusters = kmeans.predict(X_test_scaled_df)\n",
    "X_test_scaled_clustered_df = X_test_scaled_df.copy(deep=True)\n",
    "X_test_scaled_clustered_df[\"Cluster\"] = test_clusters\n",
    "test_dummy_columns = pd.get_dummies(X_test_scaled_clustered_df['Cluster'], prefix='class')\n",
    "X_test_scaled_clustered_df = pd.concat([X_test_scaled_clustered_df, test_dummy_columns], axis=1)\n",
    "X_test_scaled_clustered_df = X_test_scaled_clustered_df.drop(\"Cluster\", axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T07:45:06.126294453Z",
     "start_time": "2024-01-20T07:45:04.660068386Z"
    }
   },
   "id": "78f54d7b40ffb96d",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_scaled_clustered_float64_df = X_scaled_clustered_df.astype(dtype=float)\n",
    "X_test_scaled_clustered_float64_df = X_test_scaled_clustered_df.astype(dtype=float)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T07:45:06.169737968Z",
     "start_time": "2024-01-20T07:45:06.128545306Z"
    }
   },
   "id": "99418a39a7b27a0f",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 31/42 [00:06<00:03,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileRegressor model failed to execute\n",
      "Solver interior-point is not anymore available in SciPy >= 1.11.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:08<00:00,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2813\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 36977500.275990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               Adjusted R-Squared  R-Squared        RMSE  \\\nModel                                                                      \nExtraTreesRegressor                          0.87       0.88  4821200.67   \nGradientBoostingRegressor                    0.87       0.88  4861268.94   \nRandomForestRegressor                        0.86       0.87  4979378.80   \nHistGradientBoostingRegressor                0.86       0.87  5009826.73   \nLGBMRegressor                                0.86       0.87  5050890.07   \nBaggingRegressor                             0.86       0.87  5082296.41   \nSGDRegressor                                 0.85       0.85  5295384.27   \nOrthogonalMatchingPursuit                    0.85       0.85  5302477.10   \nLassoCV                                      0.84       0.85  5316574.29   \nRidgeCV                                      0.84       0.85  5347891.21   \nRidge                                        0.84       0.85  5347891.21   \nLassoLarsIC                                  0.84       0.85  5351317.21   \nLassoLars                                    0.84       0.85  5362803.26   \nLasso                                        0.84       0.85  5362803.29   \nLassoLarsCV                                  0.84       0.85  5362806.52   \nTransformedTargetRegressor                   0.84       0.85  5362806.52   \nLinearRegression                             0.84       0.85  5362806.52   \nOrthogonalMatchingPursuitCV                  0.84       0.85  5371794.00   \nXGBRegressor                                 0.83       0.84  5490464.04   \nPoissonRegressor                             0.82       0.83  5651343.67   \nElasticNet                                   0.81       0.82  5871574.06   \nKNeighborsRegressor                          0.81       0.82  5938499.51   \nHuberRegressor                               0.80       0.81  5971691.90   \nLarsCV                                       0.79       0.80  6166956.29   \nTweedieRegressor                             0.76       0.78  6572457.57   \nRANSACRegressor                              0.76       0.77  6638077.64   \nGammaRegressor                               0.71       0.73  7214120.88   \nDecisionTreeRegressor                        0.69       0.70  7536372.44   \nExtraTreeRegressor                           0.64       0.66  8097868.92   \nAdaBoostRegressor                            0.57       0.59  8883839.65   \nElasticNetCV                                -0.06      -0.00 13877248.49   \nBayesianRidge                               -0.06      -0.00 13879526.71   \nDummyRegressor                              -0.06      -0.00 13879526.71   \nNuSVR                                       -0.12      -0.06 14295125.05   \nSVR                                         -0.17      -0.10 14572832.44   \nGaussianProcessRegressor                    -0.81      -0.71 18156666.10   \nLars                                        -6.34      -5.94 36539330.56   \nKernelRidge                                 -6.65      -6.24 37314346.10   \nPassiveAggressiveRegressor                  -7.11      -6.67 38414856.09   \nLinearSVR                                   -7.76      -7.28 39924565.13   \nMLPRegressor                                -7.76      -7.28 39924691.05   \n\n                               Time Taken  \nModel                                      \nExtraTreesRegressor                  0.54  \nGradientBoostingRegressor            0.60  \nRandomForestRegressor                1.20  \nHistGradientBoostingRegressor        0.22  \nLGBMRegressor                        0.06  \nBaggingRegressor                     0.13  \nSGDRegressor                         0.02  \nOrthogonalMatchingPursuit            0.01  \nLassoCV                              0.19  \nRidgeCV                              0.03  \nRidge                                0.01  \nLassoLarsIC                          0.03  \nLassoLars                            0.02  \nLasso                                0.05  \nLassoLarsCV                          0.05  \nTransformedTargetRegressor           0.01  \nLinearRegression                     0.01  \nOrthogonalMatchingPursuitCV          0.01  \nXGBRegressor                         0.23  \nPoissonRegressor                     1.50  \nElasticNet                           0.03  \nKNeighborsRegressor                  0.02  \nHuberRegressor                       0.02  \nLarsCV                               0.05  \nTweedieRegressor                     0.60  \nRANSACRegressor                      0.05  \nGammaRegressor                       0.33  \nDecisionTreeRegressor                0.03  \nExtraTreeRegressor                   0.03  \nAdaBoostRegressor                    0.25  \nElasticNetCV                         0.14  \nBayesianRidge                        0.01  \nDummyRegressor                       0.01  \nNuSVR                                0.14  \nSVR                                  0.16  \nGaussianProcessRegressor             0.47  \nLars                                 0.02  \nKernelRidge                          0.11  \nPassiveAggressiveRegressor           0.09  \nLinearSVR                            0.01  \nMLPRegressor                         1.09  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adjusted R-Squared</th>\n      <th>R-Squared</th>\n      <th>RMSE</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.87</td>\n      <td>0.88</td>\n      <td>4821200.67</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.87</td>\n      <td>0.88</td>\n      <td>4861268.94</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.86</td>\n      <td>0.87</td>\n      <td>4979378.80</td>\n      <td>1.20</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingRegressor</th>\n      <td>0.86</td>\n      <td>0.87</td>\n      <td>5009826.73</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.86</td>\n      <td>0.87</td>\n      <td>5050890.07</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>BaggingRegressor</th>\n      <td>0.86</td>\n      <td>0.87</td>\n      <td>5082296.41</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>SGDRegressor</th>\n      <td>0.85</td>\n      <td>0.85</td>\n      <td>5295384.27</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuit</th>\n      <td>0.85</td>\n      <td>0.85</td>\n      <td>5302477.10</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LassoCV</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5316574.29</td>\n      <td>0.19</td>\n    </tr>\n    <tr>\n      <th>RidgeCV</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5347891.21</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5347891.21</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LassoLarsIC</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5351317.21</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>LassoLars</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5362803.26</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5362803.29</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>LassoLarsCV</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5362806.52</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>TransformedTargetRegressor</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5362806.52</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5362806.52</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuitCV</th>\n      <td>0.84</td>\n      <td>0.85</td>\n      <td>5371794.00</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.83</td>\n      <td>0.84</td>\n      <td>5490464.04</td>\n      <td>0.23</td>\n    </tr>\n    <tr>\n      <th>PoissonRegressor</th>\n      <td>0.82</td>\n      <td>0.83</td>\n      <td>5651343.67</td>\n      <td>1.50</td>\n    </tr>\n    <tr>\n      <th>ElasticNet</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>5871574.06</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>5938499.51</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>HuberRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>5971691.90</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LarsCV</th>\n      <td>0.79</td>\n      <td>0.80</td>\n      <td>6166956.29</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>TweedieRegressor</th>\n      <td>0.76</td>\n      <td>0.78</td>\n      <td>6572457.57</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>RANSACRegressor</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>6638077.64</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>GammaRegressor</th>\n      <td>0.71</td>\n      <td>0.73</td>\n      <td>7214120.88</td>\n      <td>0.33</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.69</td>\n      <td>0.70</td>\n      <td>7536372.44</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeRegressor</th>\n      <td>0.64</td>\n      <td>0.66</td>\n      <td>8097868.92</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.57</td>\n      <td>0.59</td>\n      <td>8883839.65</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>ElasticNetCV</th>\n      <td>-0.06</td>\n      <td>-0.00</td>\n      <td>13877248.49</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>BayesianRidge</th>\n      <td>-0.06</td>\n      <td>-0.00</td>\n      <td>13879526.71</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DummyRegressor</th>\n      <td>-0.06</td>\n      <td>-0.00</td>\n      <td>13879526.71</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NuSVR</th>\n      <td>-0.12</td>\n      <td>-0.06</td>\n      <td>14295125.05</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>-0.17</td>\n      <td>-0.10</td>\n      <td>14572832.44</td>\n      <td>0.16</td>\n    </tr>\n    <tr>\n      <th>GaussianProcessRegressor</th>\n      <td>-0.81</td>\n      <td>-0.71</td>\n      <td>18156666.10</td>\n      <td>0.47</td>\n    </tr>\n    <tr>\n      <th>Lars</th>\n      <td>-6.34</td>\n      <td>-5.94</td>\n      <td>36539330.56</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>KernelRidge</th>\n      <td>-6.65</td>\n      <td>-6.24</td>\n      <td>37314346.10</td>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveRegressor</th>\n      <td>-7.11</td>\n      <td>-6.67</td>\n      <td>38414856.09</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>LinearSVR</th>\n      <td>-7.76</td>\n      <td>-7.28</td>\n      <td>39924565.13</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>-7.76</td>\n      <td>-7.28</td>\n      <td>39924691.05</td>\n      <td>1.09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg1 = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "_, predictions1 = reg1.fit(X_scaled_clustered_df, X_test_scaled_clustered_df, y1, test_y1)\n",
    "predictions1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T07:45:15.269360833Z",
     "start_time": "2024-01-20T07:45:06.653103458Z"
    }
   },
   "id": "5e19c3164cba31b7",
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p> <b> Міні-висновок: </b> Кластеризація ніяк не впливає на найкращий регресор</p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1589f81803c1e4d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4> 5. ADD GRNN1, GRNN2, GRNN3 to count variables multi-dependence</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603c86fedfce184f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing warm start...\n",
      "Warm start concluded. The optimum isotropic sigma is [0.07759374]\n",
      "Gradient search concluded. The optimum sigma is [0.06431615 0.02068963 0.01609462 0.71081714 5.41707759 1.17477872\n",
      " 0.03846657 1.03158035 0.33788862 0.97183265 0.43128598 0.82558112\n",
      " 0.078      0.078      0.078      0.078      0.078      0.078\n",
      " 0.078      0.078      0.078      0.078     ]\n",
      "Sigma1 =  [0.06431615 0.02068963 0.01609462 0.71081714 5.41707759 1.17477872\n",
      " 0.03846657 1.03158035 0.33788862 0.97183265 0.43128598 0.82558112\n",
      " 0.078      0.078      0.078      0.078      0.078      0.078\n",
      " 0.078      0.078      0.078      0.078     ]\n",
      "Executing warm start...\n",
      "Warm start concluded. The optimum isotropic sigma is [0.06845064]\n",
      "Gradient search concluded. The optimum sigma is [1.56418311e-01 5.03200655e-02 6.08234926e-02 2.86035356e+01\n",
      " 8.94409133e+00 6.19084082e+01 5.89682179e+01 3.74483815e+01\n",
      " 1.07253541e+01 4.18497662e-01 2.27703347e-02 9.64677829e-02\n",
      " 6.80000000e-02 6.80000000e-02 6.80000000e-02 6.80000000e-02\n",
      " 6.80000000e-02 6.80000000e-02 6.80000000e-02 6.80000000e-02\n",
      " 6.80000000e-02 6.80000000e-02]\n",
      "Sigma2 =  [1.56418311e-01 5.03200655e-02 6.08234926e-02 2.86035356e+01\n",
      " 8.94409133e+00 6.19084082e+01 5.89682179e+01 3.74483815e+01\n",
      " 1.07253541e+01 4.18497662e-01 2.27703347e-02 9.64677829e-02\n",
      " 6.80000000e-02 6.80000000e-02 6.80000000e-02 6.80000000e-02\n",
      " 6.80000000e-02 6.80000000e-02 6.80000000e-02 6.80000000e-02\n",
      " 6.80000000e-02 6.80000000e-02]\n",
      "Executing warm start...\n",
      "Warm start concluded. The optimum isotropic sigma is [0.08826448]\n",
      "Gradient search concluded. The optimum sigma is [8.77014330e-02 4.31754017e-02 5.07389412e-02 1.66091319e+01\n",
      " 2.75110001e+00 5.31614473e-02 3.90579312e-02 2.44271175e+01\n",
      " 4.29628537e+01 4.00393748e+01 5.19663949e-02 1.04484040e-01\n",
      " 8.80000000e-02 8.80000000e-02 8.80000000e-02 8.80000000e-02\n",
      " 8.80000000e-02 8.80000000e-02 8.80000000e-02 8.80000000e-02\n",
      " 8.80000000e-02 8.80000000e-02]\n",
      "Sigma3 =  [8.77014330e-02 4.31754017e-02 5.07389412e-02 1.66091319e+01\n",
      " 2.75110001e+00 5.31614473e-02 3.90579312e-02 2.44271175e+01\n",
      " 4.29628537e+01 4.00393748e+01 5.19663949e-02 1.04484040e-01\n",
      " 8.80000000e-02 8.80000000e-02 8.80000000e-02 8.80000000e-02\n",
      " 8.80000000e-02 8.80000000e-02 8.80000000e-02 8.80000000e-02\n",
      " 8.80000000e-02 8.80000000e-02]\n"
     ]
    }
   ],
   "source": [
    "# 4. train GRNN for each dataset\n",
    "grnn1 = GRNN()\n",
    "grnn1.fit(X_scaled_clustered_df, y1)\n",
    "y_pred1 = grnn1.predict(X_scaled_clustered_df)\n",
    "print(\"Sigma1 = \", grnn1.sigma)\n",
    "\n",
    "grnn2 = GRNN()\n",
    "grnn2.fit(X_scaled_clustered_df, y2)\n",
    "y_pred2 = grnn2.predict(X_scaled_clustered_df)\n",
    "print(\"Sigma2 = \", grnn2.sigma)\n",
    "\n",
    "grnn3 = GRNN()\n",
    "grnn3.fit(X_scaled_clustered_df, y3)\n",
    "y_pred3 = grnn3.predict(X_scaled_clustered_df)\n",
    "print(\"Sigma3 = \", grnn3.sigma)\n",
    "\n",
    "X1, X2, X3 = X_scaled_clustered_df.copy(deep=True), X_scaled_clustered_df.copy(deep=True), X_scaled_clustered_df.copy(deep=True)\n",
    "\n",
    "X3[\"GRNN1\"] = X2[\"GRNN1\"] = X1[\"GRNN1\"] = y_pred1\n",
    "X3[\"GRNN2\"] = X2[\"GRNN2\"] = X1[\"GRNN2\"] = y_pred2\n",
    "X3[\"GRNN3\"] = X2[\"GRNN3\"] = X1[\"GRNN3\"] = y_pred3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:00:40.191664458Z",
     "start_time": "2024-01-20T07:48:20.319346890Z"
    }
   },
   "id": "5c62cec807022c8c",
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 6. Scale one more time to count newly added GRNN features </h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f3a0aa8071356de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# X1\n",
    "X1_scaler = MaxAbsScaler()\n",
    "X1_scaler.fit(X1)\n",
    "X1_scaled = X1_scaler.transform(X1)\n",
    "\n",
    "# X2\n",
    "X2_scaler = MaxAbsScaler()\n",
    "X2_scaler.fit(X2)\n",
    "X2_scaled = X2_scaler.transform(X2)\n",
    " \n",
    "# X3\n",
    "X3_scaler = MaxAbsScaler()\n",
    "X3_scaler.fit(X1)\n",
    "X3_scaled = X3_scaler.transform(X3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:06:05.600910802Z",
     "start_time": "2024-01-20T08:06:05.567633314Z"
    }
   },
   "id": "8a5f6cc671761b4b",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_y_pred1 = grnn1.predict(X_test_scaled_clustered_df)\n",
    "test_y_pred2 = grnn2.predict(X_test_scaled_clustered_df)\n",
    "test_y_pred3 = grnn3.predict(X_test_scaled_clustered_df)\n",
    "test_X1, test_X2, test_X3 = X_test_scaled_clustered_df.copy(deep=True), X_test_scaled_clustered_df.copy(deep=True), X_test_scaled_clustered_df.copy(deep=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:06:07.959839282Z",
     "start_time": "2024-01-20T08:06:07.885384279Z"
    }
   },
   "id": "45294852c1a8b6b1",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_X3[\"GRNN1\"] = test_X2[\"GRNN1\"] = test_X1[\"GRNN1\"] = test_y_pred1\n",
    "test_X3[\"GRNN2\"] = test_X2[\"GRNN2\"] = test_X1[\"GRNN2\"] = test_y_pred2\n",
    "test_X3[\"GRNN3\"] = test_X2[\"GRNN3\"] = test_X1[\"GRNN3\"] = test_y_pred3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:06:09.501177795Z",
     "start_time": "2024-01-20T08:06:09.480571860Z"
    }
   },
   "id": "c8b02e1f5fea9cd8",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test datasets (scaled)\n",
    "test_X1_scaled = X1_scaler.transform(test_X1)\n",
    "test_X2_scaled = X2_scaler.transform(test_X2) \n",
    "test_X3_scaled = X3_scaler.transform(test_X3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:06:09.802540080Z",
     "start_time": "2024-01-20T08:06:09.782305056Z"
    }
   },
   "id": "3c83d8972436fc57",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:07<00:00,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3608\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 2376948.061796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               Adjusted R-Squared  R-Squared        RMSE  \\\nModel                                                                      \nDecisionTreeRegressor                        0.81       0.82  6388993.49   \nBaggingRegressor                             0.81       0.82  6446242.06   \nRandomForestRegressor                        0.81       0.82  6459960.26   \nExtraTreesRegressor                          0.80       0.82  6522479.90   \nOrthogonalMatchingPursuit                    0.80       0.81  6547122.79   \nOrthogonalMatchingPursuitCV                  0.80       0.81  6547122.79   \nLGBMRegressor                                0.80       0.81  6563509.22   \nSGDRegressor                                 0.80       0.81  6571018.69   \nLassoCV                                      0.80       0.81  6572890.31   \nLassoLarsCV                                  0.80       0.81  6573341.29   \nLassoLarsIC                                  0.80       0.81  6573813.54   \nLarsCV                                       0.80       0.81  6575546.22   \nRidgeCV                                      0.80       0.81  6588024.73   \nRidge                                        0.80       0.81  6588024.73   \nLasso                                        0.80       0.81  6590490.29   \nLinearRegression                             0.80       0.81  6590492.44   \nTransformedTargetRegressor                   0.80       0.81  6590492.44   \nLassoLars                                    0.80       0.81  6590492.85   \nHistGradientBoostingRegressor                0.80       0.81  6591332.57   \nRANSACRegressor                              0.80       0.81  6594677.84   \nAdaBoostRegressor                            0.80       0.81  6622172.35   \nGradientBoostingRegressor                    0.80       0.81  6634058.53   \nXGBRegressor                                 0.79       0.81  6687598.53   \nKernelRidge                                  0.77       0.78  7105424.94   \nHuberRegressor                               0.76       0.78  7159702.65   \nExtraTreeRegressor                           0.76       0.77  7240310.62   \nElasticNet                                   0.75       0.77  7303338.73   \nTweedieRegressor                             0.69       0.71  8165594.25   \nKNeighborsRegressor                          0.68       0.70  8250714.87   \nGaussianProcessRegressor                     0.37       0.41 11642137.99   \nPassiveAggressiveRegressor                   0.20       0.25 13143489.22   \nElasticNetCV                                -0.07      -0.00 15186743.54   \nBayesianRidge                               -0.07      -0.00 15188667.82   \nDummyRegressor                              -0.07      -0.00 15188667.82   \nNuSVR                                       -0.08      -0.01 15237758.36   \nSVR                                         -0.08      -0.01 15278056.80   \nLinearSVR                                   -0.11      -0.04 15483238.75   \nMLPRegressor                                -0.11      -0.04 15484542.09   \nLars                                        -5.87      -5.44 38505075.78   \n\n                               Time Taken  \nModel                                      \nDecisionTreeRegressor                0.04  \nBaggingRegressor                     0.15  \nRandomForestRegressor                1.41  \nExtraTreesRegressor                  0.81  \nOrthogonalMatchingPursuit            0.01  \nOrthogonalMatchingPursuitCV          0.01  \nLGBMRegressor                        0.08  \nSGDRegressor                         0.04  \nLassoCV                              0.17  \nLassoLarsCV                          0.08  \nLassoLarsIC                          0.05  \nLarsCV                               0.06  \nRidgeCV                              0.06  \nRidge                                0.01  \nLasso                                0.07  \nLinearRegression                     0.01  \nTransformedTargetRegressor           0.01  \nLassoLars                            0.03  \nHistGradientBoostingRegressor        0.23  \nRANSACRegressor                      0.05  \nAdaBoostRegressor                    0.24  \nGradientBoostingRegressor            0.73  \nXGBRegressor                         0.25  \nKernelRidge                          0.11  \nHuberRegressor                       0.02  \nExtraTreeRegressor                   0.02  \nElasticNet                           0.01  \nTweedieRegressor                     0.86  \nKNeighborsRegressor                  0.01  \nGaussianProcessRegressor             0.36  \nPassiveAggressiveRegressor           0.19  \nElasticNetCV                         0.08  \nBayesianRidge                        0.01  \nDummyRegressor                       0.01  \nNuSVR                                0.15  \nSVR                                  0.20  \nLinearSVR                            0.03  \nMLPRegressor                         1.17  \nLars                                 0.03  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adjusted R-Squared</th>\n      <th>R-Squared</th>\n      <th>RMSE</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>6388993.49</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>BaggingRegressor</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>6446242.06</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.81</td>\n      <td>0.82</td>\n      <td>6459960.26</td>\n      <td>1.41</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.80</td>\n      <td>0.82</td>\n      <td>6522479.90</td>\n      <td>0.81</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuit</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6547122.79</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>OrthogonalMatchingPursuitCV</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6547122.79</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6563509.22</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>SGDRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6571018.69</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>LassoCV</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6572890.31</td>\n      <td>0.17</td>\n    </tr>\n    <tr>\n      <th>LassoLarsCV</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6573341.29</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>LassoLarsIC</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6573813.54</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>LarsCV</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6575546.22</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>RidgeCV</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6588024.73</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6588024.73</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6590490.29</td>\n      <td>0.07</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6590492.44</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>TransformedTargetRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6590492.44</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LassoLars</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6590492.85</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6591332.57</td>\n      <td>0.23</td>\n    </tr>\n    <tr>\n      <th>RANSACRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6594677.84</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6622172.35</td>\n      <td>0.24</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.80</td>\n      <td>0.81</td>\n      <td>6634058.53</td>\n      <td>0.73</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.79</td>\n      <td>0.81</td>\n      <td>6687598.53</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>KernelRidge</th>\n      <td>0.77</td>\n      <td>0.78</td>\n      <td>7105424.94</td>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>HuberRegressor</th>\n      <td>0.76</td>\n      <td>0.78</td>\n      <td>7159702.65</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeRegressor</th>\n      <td>0.76</td>\n      <td>0.77</td>\n      <td>7240310.62</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>ElasticNet</th>\n      <td>0.75</td>\n      <td>0.77</td>\n      <td>7303338.73</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>TweedieRegressor</th>\n      <td>0.69</td>\n      <td>0.71</td>\n      <td>8165594.25</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.68</td>\n      <td>0.70</td>\n      <td>8250714.87</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>GaussianProcessRegressor</th>\n      <td>0.37</td>\n      <td>0.41</td>\n      <td>11642137.99</td>\n      <td>0.36</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveRegressor</th>\n      <td>0.20</td>\n      <td>0.25</td>\n      <td>13143489.22</td>\n      <td>0.19</td>\n    </tr>\n    <tr>\n      <th>ElasticNetCV</th>\n      <td>-0.07</td>\n      <td>-0.00</td>\n      <td>15186743.54</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>BayesianRidge</th>\n      <td>-0.07</td>\n      <td>-0.00</td>\n      <td>15188667.82</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DummyRegressor</th>\n      <td>-0.07</td>\n      <td>-0.00</td>\n      <td>15188667.82</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NuSVR</th>\n      <td>-0.08</td>\n      <td>-0.01</td>\n      <td>15237758.36</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>-0.08</td>\n      <td>-0.01</td>\n      <td>15278056.80</td>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>LinearSVR</th>\n      <td>-0.11</td>\n      <td>-0.04</td>\n      <td>15483238.75</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>-0.11</td>\n      <td>-0.04</td>\n      <td>15484542.09</td>\n      <td>1.17</td>\n    </tr>\n    <tr>\n      <th>Lars</th>\n      <td>-5.87</td>\n      <td>-5.44</td>\n      <td>38505075.78</td>\n      <td>0.03</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2 = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "_, predictions2 = reg2.fit(X2_scaled, test_X2_scaled, y2, test_y2)\n",
    "predictions2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:07:42.502125246Z",
     "start_time": "2024-01-20T08:07:34.565411868Z"
    }
   },
   "id": "da797275f0e7e059",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "646a473ab83a1e79"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
